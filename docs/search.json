[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Phu’s Datascience Project",
    "section": "",
    "text": "Pomona College | Claremont, CA\nB. A in Computer Science | Aug. 2024 - May 2028"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Phu’s Datascience Project",
    "section": "",
    "text": "Pomona College | Claremont, CA\nB. A in Computer Science | Aug. 2024 - May 2028"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Phu’s Datascience Project",
    "section": "About Me",
    "text": "About Me\nHi! I’m Phu, a Sophmore at Pomona College. I’m originally from Nha Trang, Vietnam, but live in the states now (Richmond, VA). One of my greatest passions in life is music. I didn’t have the opportunity growing up, but after getting a job, I’ve been able to buy the instruments and have been learning guitar, piano, and ukulele. I’m taking voice lessons at Pomona currently! You’ll find my Spotify linked below my profile picture, if you’re interested in what music I listen to. Because music is such a big part of my life, I decided to do my TidyTuesday analyses on Eurovision and Spotify statistics. Hope you enjoy my short page!"
  },
  {
    "objectID": "dveu.html",
    "href": "dveu.html",
    "title": "Eurovision Song Contest",
    "section": "",
    "text": "The Eurovision dataset contains a filed called eurovision.csv, and is a collection of data from Eurovision contests since its inception in 1956, with the exception of the 2020 Covid Year. The dataset represents information for all contestants for each year and by round, but it is important to note that changes in the scoring system have occurred throughout different points in time.For example, semi-final rounds were not introduced until 2005. Variables include the event, the host city, the artist country of origin, their song, the points they accumulated, and their rank, among others. I listened to a lot of Eurovision classics without having known before, so it was a pleasant surprise to do an analysis of the top Eurovision countries. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dveu.html#introduction",
    "href": "dveu.html#introduction",
    "title": "Eurovision Song Contest",
    "section": "",
    "text": "The Eurovision dataset contains a filed called eurovision.csv, and is a collection of data from Eurovision contests since its inception in 1956, with the exception of the 2020 Covid Year. The dataset represents information for all contestants for each year and by round, but it is important to note that changes in the scoring system have occurred throughout different points in time.For example, semi-final rounds were not introduced until 2005. Variables include the event, the host city, the artist country of origin, their song, the points they accumulated, and their rank, among others. I listened to a lot of Eurovision classics without having known before, so it was a pleasant surprise to do an analysis of the top Eurovision countries. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dveu.html#code-and-graph",
    "href": "dveu.html#code-and-graph",
    "title": "Eurovision Song Contest",
    "section": "Code and Graph",
    "text": "Code and Graph\n\n# Top 10 countries by cumulative total points since 2005\ntop_countries &lt;- euro |&gt;\n  filter(year &gt;= 2005) |&gt;\n  group_by(artist_country) |&gt;\n  summarize(sum_total_points = sum(total_points, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(sum_total_points)) |&gt;\n  slice_head(n = 10) # Cuts the list into just the top 10 countries.\n\n# Plot\nggplot(top_countries, aes(x = reorder(artist_country, sum_total_points),\n                          y = sum_total_points)) +\n  geom_col(fill = \"cornflowerblue\") + \n  geom_text(aes(label = sum_total_points),\n            hjust = -.1,\n            size = 2) +\n  coord_flip() +\n  labs(title = \"Top 10 Eurovision Countries by Total Points (2005–Present)\",\n       x = \"Country\",\n       y = \"Total Points\") +\n  theme_light()"
  },
  {
    "objectID": "dveu.html#analysis",
    "href": "dveu.html#analysis",
    "title": "Eurovision Song Contest",
    "section": "Analysis",
    "text": "Analysis\nThe horizontal bar chart shows the total Eurovision points earned by countries since 2005 (the introduction of the semi-final system, to make results more comparable to the modern contest). Sweden dominates the charts with 5669 points, followed by Ukraine with 4998 and Russia with 3977. Norway, Greece, Azerbaijan, and Moldova show solid performances in their Eurovision entries. In particular, Azerbaijan only entered the competition in 2008, so 3 years late, but is 6th in total accumulated points, with 3528– an extraordinary result.The following countries are followed by Serbia, then the Netherlands, and lastly Bulgaria, with 2945 points. Eastern European countries dominate the charts, which is surprising, given that the Eurovision “Big-Five,” all Western European countries (U.K., Spain, Germany, France, Italy) are now automatically qualified to the finals despite not scoring the highest. Perhaps, it’s time for a new “Big Five?” Or, do the countries not try as hard to score points, knowing that they are guaranteed finalists due to their financial backing of the competition? It is important to acknowledge that there have been some disbanded countries (countries that no longer exist, such as Serbia and Montenegro), or inactive countries, which affect the total accumulated score of the country (especially in a split like Serbia and Montenegro). I decided that it was better to still continue with total accumulated points compared to average points, because Serbia and Montenegro for example participated in a single contest, placed 2nd, and while is obviously an outlier, “rules” the dataset as the best country by average points."
  },
  {
    "objectID": "dveu.html#link-and-credits",
    "href": "dveu.html#link-and-credits",
    "title": "Eurovision Song Contest",
    "section": "Link and Credits",
    "text": "Link and Credits\nThe data is provided by Eurovision, and Tanya Shapiro and Bob Rudis shared methods to clean and scrape the data. The link to the dataset can be accessed here: TidyTuesday Eurovision Dataset. From https://eurovision.tv/."
  },
  {
    "objectID": "AnimalCrossing.html",
    "href": "AnimalCrossing.html",
    "title": "Animal Crossing",
    "section": "",
    "text": "For this week’s project, I decided to use the Animal Crossing: New Horizons Villager dataset from TinyTuesday, which was originally compiled from VillagerDB. In my analysis, I asked whether or not a villager’s birth half of the year (born between Jan. - June vs. born between July-Dec.) is associated with their personality– more specifically, if they are cranky or not. My hypothesis question would be, are villagers born in the first half of the year more or less likely to be cranky than those who are born later. I wanted to do this experiment because I realized most of my friends are born in the 2nd half of the year, more specifically Sept.-Nov., and I’m one of the only people born in the first half for some odd reason (Jan.). I thought it would be a great way to ragebait them and prove they are “just like the data” if I’m able to prove that people who are born in the latter half of the year are indeed more likely to be cranky than people born in the first half of the year."
  },
  {
    "objectID": "AnimalCrossing.html#introduction",
    "href": "AnimalCrossing.html#introduction",
    "title": "Animal Crossing",
    "section": "",
    "text": "For this week’s project, I decided to use the Animal Crossing: New Horizons Villager dataset from TinyTuesday, which was originally compiled from VillagerDB. In my analysis, I asked whether or not a villager’s birth half of the year (born between Jan. - June vs. born between July-Dec.) is associated with their personality– more specifically, if they are cranky or not. My hypothesis question would be, are villagers born in the first half of the year more or less likely to be cranky than those who are born later. I wanted to do this experiment because I realized most of my friends are born in the 2nd half of the year, more specifically Sept.-Nov., and I’m one of the only people born in the first half for some odd reason (Jan.). I thought it would be a great way to ragebait them and prove they are “just like the data” if I’m able to prove that people who are born in the latter half of the year are indeed more likely to be cranky than people born in the first half of the year."
  },
  {
    "objectID": "AnimalCrossing.html#simulation-details-what-i-plan-to-do",
    "href": "AnimalCrossing.html#simulation-details-what-i-plan-to-do",
    "title": "Animal Crossing",
    "section": "Simulation Details (What I plan to do)",
    "text": "Simulation Details (What I plan to do)\nTo test this, I conduct a permutation test simulating the null hypothesis (Ho) that there is no relationship between birth half and personality. My alternative hypothesis (Ha) says that people born in the latter half of the year, from July-Dec., are more likely to be cranky when compared to the first half of the year people. To derive this, I’ll first use lubridate to derive each villager’s birth month from the birthday variable and categorize them into Jan.-June or July-Dec. Then, I’ll calculate the observed differences in proportions of Cranky villagers between the two halves of the year. I wrote a function to randomly permute birth-half labels, recompute the difference in proportions, and return the value. Then, I used map_dbl() to repeat the process 1,000 times to generate a null-distribution of differences under random label assignment to estimate an empirical p-value by comparing the observed difference to the distribution of permuted differences. The first graph I’ll create shows the proportion of each personality across the two birth halves, to help visualize the observed data. The second graph is a histogram of the permutation distribution which shows what kind of proportion differences that should be expected if there was no true relationship between birth-half and crankiness."
  },
  {
    "objectID": "AnimalCrossing.html#data-source",
    "href": "AnimalCrossing.html#data-source",
    "title": "Animal Crossing",
    "section": "Data Source",
    "text": "Data Source\nThe data for the information about villagers comes from the TidyTuesday Animal Crossing: New Horizon dataset, available through the TidyTuesday Github Repository:Github. The information comes from Villager DB, which is a community-maintained database that compiles detailed profiles of all Animal Crossing villagers. I am currently unable to access this datasource or locate the individuals that put the data into a presentable form. Both the website VillagerDB and the Github repository are down, and after a quick Reddit search, I learned that they’ve been taken down recently, and I don’t know when it is coming back. For the original data in the game, the data was created by the developers, Nintendo, and the producer who “created” the Animal Crossing games would be Hisashi Nogami. More details about Animal Crossing and characters can be officially sourced to their website."
  },
  {
    "objectID": "AnimalCrossing.html#code",
    "href": "AnimalCrossing.html#code",
    "title": "Animal Crossing",
    "section": "Code",
    "text": "Code\n\nobserved_proportion_diff &lt;- function(target_personality = \"Cranky\") {\n  first_half &lt;- villagers |&gt;\n    filter(birth_half == \"Jan-Jun\", personality == target_personality) |&gt;\n    nrow() /\n    nrow(villagers |&gt; filter(birth_half == \"Jan-Jun\"))\n\n  \n  latter_half &lt;- villagers |&gt;\n    filter(birth_half == \"Jul-Dec\", personality == target_personality) |&gt;\n    nrow() /\n    nrow(villagers |&gt; filter(birth_half == \"Jul-Dec\"))\n\n  first_half - latter_half\n}\n\n\nobserved_proportion_diff(\"Cranky\")\n\n[1] -0.07486631\n\n\n\none_permutation &lt;- function(villagers, target_personality = \"Cranky\") {\n  villagers_permutation &lt;- villagers |&gt;\n    filter(!is.na(birth_half)) |&gt;\n    mutate(birth_half = sample(birth_half))\n  \n  \nfirst_half &lt;- villagers_permutation |&gt;\n  filter(birth_half == \"Jan-Jun\", personality == target_personality) |&gt;\n  nrow() /\n  (villagers_permutation |&gt;\n      filter(birth_half == \"Jan-Jun\") |&gt;\n      nrow())\n\n\nlatter_half &lt;- villagers_permutation |&gt;\n  filter(birth_half == \"Jul-Dec\", personality == target_personality) |&gt;\n  nrow() /\n  (villagers_permutation |&gt;\n      filter(birth_half == \"Jul-Dec\") |&gt;\n      nrow())\n  \n\nfirst_half - latter_half\n}\n\nset.seed(47)\n\npermutation_diff &lt;- map_dbl(1:1000, ~ one_permutation(villagers, \"Cranky\"))\n\nobserved_diff &lt;- observed_proportion_diff(\"Cranky\")\n\np_value &lt;- mean(abs(permutation_diff) &gt;= abs(observed_diff))\np_value\n\n[1] 0.039\n\n\n\ncranky_proportion &lt;- villagers |&gt;\n  filter(!is.na(birth_half)) |&gt;\n  group_by(birth_half) |&gt;\n  summarize(\n    cranky_prop = mean(personality == \"Cranky\")\n  )\n\ncranky_proportion |&gt;\n  ggplot(aes(x = birth_half, y = cranky_prop, fill = birth_half)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Proportion of Cranky Villagers by Birth Half\",\n    x = \"Birth Half of the Year\",\n    y = \"Proportion Cranky\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis bar chart shows the observed proportions of crankiness into two bins (Jan.-June births and July-Dec. births), which showed a difference of around 7.5 percentage points in proportion of crankiness in the villagers. Villagers born in the first half of the year have about 10% of their population having a cranky personality, while in the latter half of the year this proportion is around 17.5%.\n\ntibble(diff = permutation_diff) |&gt;\n  ggplot(aes(x = diff)) +\n  geom_histogram(bins = 30, fill = \"plum1\", color = \"black\") +\n  geom_vline(xintercept = observed_diff, color = \"red4\", linewidth = 1.0) +\n  labs(\n    title = \"Cranky Proportion Differences by Birth Half\",\n    x = \"Permuted difference (Jan-Jun - Jul-Dec)\",\n    y = \"Count\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis histogram displays the permutation distribution of proportion differences under the null hypothesis, with the vertical line marking the observed difference. The plot shows that the observed value lies at the edge of the simulated distribution, rather than at a 0.00 permuted difference, which provides visual evidence against the null hypothesis."
  },
  {
    "objectID": "AnimalCrossing.html#simulation-details-cont.-what-i-did-results-summary",
    "href": "AnimalCrossing.html#simulation-details-cont.-what-i-did-results-summary",
    "title": "Animal Crossing",
    "section": "Simulation Details (cont.) (What I did, Results Summary)",
    "text": "Simulation Details (cont.) (What I did, Results Summary)\nIn this simulation, I performed a permutation test to evaluate whether villagers born in the first half of the year (January–June) were more or less likely to be cranky than those born in the latter half (July–December). After categorizing villagers by their birth month and calculating the observed difference in proportions, I found that the proportion of cranky villagers was higher in the July–December group. The observed difference was approximately −0.0749, meaning the first-half group had about roughly 7.49% fewer cranky villagers on average. To test whether this difference could have arisen by chance, I simulated the null hypothesis by randomly shuffling the birth-half labels 1,000 times using my function one_permutation(), applied through map_dbl(). This generated a null distribution of differences assuming no true association between birth month and personality. The observed difference fell in the tail of this distribution, corresponding to an empirical p-value of approximately 0.039, and as 0.039 &lt; 0.05, it suggests the result is statistically significant at the 95% confidence level.\nOverall, the simulation supports the hypothesis that villagers born in the latter half of the year are slightly more likely to be cranky– we should reject Ho, as villagers born from July onwards are more likely to be cranky compared to their earlier born counterparts. Permutation testing can be used to assess non-parametric relationships between categorical variables, and in this case, it appears that people born July and afterwards are indeed more upset than those born from January thru. June ;)."
  },
  {
    "objectID": "Dickinson.html",
    "href": "Dickinson.html",
    "title": "Emily Dickinson Poems",
    "section": "",
    "text": "For this week’s project, I decided to do a full-text analysis from a collection of all of Emily Dickinson’s poems, transcribed by Jim Tinsley and made freely available on the Gutenberg project. In my analysis, I use 6 str_*() functions, those being str_count(), str_detect(), str_to_lower(), str_split(), str_extract(), and str_extract_all(). The lookaround I use is a lookbehind. I use 4 regular expressions – 1) to detect whether or not a poem contains death related words, 2) to extract any “year-like” number (4-digits), 3) during the lookbehind, and 4) to count the number of punctuation Dickinson uses. My two plots include one that graphs the top 10 most frequent words in Dickinson poems, and the second being an analysis of poem length to punctuation count– which, we should see, that the longer the poem length, the more punctuation it features."
  },
  {
    "objectID": "Dickinson.html#introduction",
    "href": "Dickinson.html#introduction",
    "title": "Emily Dickinson Poems",
    "section": "",
    "text": "For this week’s project, I decided to do a full-text analysis from a collection of all of Emily Dickinson’s poems, transcribed by Jim Tinsley and made freely available on the Gutenberg project. In my analysis, I use 6 str_*() functions, those being str_count(), str_detect(), str_to_lower(), str_split(), str_extract(), and str_extract_all(). The lookaround I use is a lookbehind. I use 4 regular expressions – 1) to detect whether or not a poem contains death related words, 2) to extract any “year-like” number (4-digits), 3) during the lookbehind, and 4) to count the number of punctuation Dickinson uses. My two plots include one that graphs the top 10 most frequent words in Dickinson poems, and the second being an analysis of poem length to punctuation count– which, we should see, that the longer the poem length, the more punctuation it features."
  },
  {
    "objectID": "Dickinson.html#data-source",
    "href": "Dickinson.html#data-source",
    "title": "Emily Dickinson Poems",
    "section": "Data Source",
    "text": "Data Source\nThe data of the Emily Dickinson Poems come from the Amherst-Statistics/DickinsonPoems GitHub repository, created by Nicholas Horton (reachable at nicholasjhorton@gmail.com), which includes transcriptions of Dickinson’s poems in the DickinsonPoems package. The link to the GitHub repo can be accessed via the following: Github.The dataset comes from Project Gutenberg, which is an e-Library of over 75,000 eBooks. The author of the poems, is, well, Emily Dickinson, and the transcriptions were produced by Jim Tinsley, who can be contacted at jtinsley@pobox.com. These poems can be accesed here: Gutenberg."
  },
  {
    "objectID": "Dickinson.html#code",
    "href": "Dickinson.html#code",
    "title": "Emily Dickinson Poems",
    "section": "Code",
    "text": "Code\n\n# Regular Expressions\n\ndeath_regexp &lt;- \"\\\\b(death|die|grave|funeral|corpse|end|demise|passing)\\\\b\"\n\nyear_regexp &lt;- \"\\\\b(10|11|12|13|14|15|16|17|18|19|20)\\\\d{2}\\\\b\"\n\n# LookAround (Look Behind)\n\nafter_word_my_regexp &lt;- \"(?&lt;=\\\\bmy\\\\s)[A-Za-z']+\"\n\npoems &lt;- poems |&gt;\n  mutate(\n    \n    punctuation_count = str_count(text, \"[[:punct:]]\"),\n    mentions_death = str_detect(str_to_lower(text), death_regexp),\n    years_found = str_extract_all(text, year_regexp),\n    after_word_my_regexp = str_extract(text, after_word_my_regexp)\n          \n  )\n\npoems |&gt; select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |&gt; head(10)\n\n# A tibble: 10 × 6\n   poem_id           n_lines punctuation_count mentions_death years_found\n   &lt;chr&gt;               &lt;int&gt;             &lt;int&gt; &lt;lgl&gt;          &lt;list&gt;     \n 1 gutenberg1.txt001      26                26 FALSE          &lt;chr [0]&gt;  \n 2 gutenberg1.txt002      15                13 FALSE          &lt;chr [0]&gt;  \n 3 gutenberg1.txt003      17                10 FALSE          &lt;chr [0]&gt;  \n 4 gutenberg1.txt004      28                35 TRUE           &lt;chr [0]&gt;  \n 5 gutenberg1.txt005      25                24 FALSE          &lt;chr [0]&gt;  \n 6 gutenberg1.txt006      13                 7 FALSE          &lt;chr [0]&gt;  \n 7 gutenberg1.txt007      17                10 FALSE          &lt;chr [0]&gt;  \n 8 gutenberg1.txt008      20                16 TRUE           &lt;chr [0]&gt;  \n 9 gutenberg1.txt009      15                11 TRUE           &lt;chr [0]&gt;  \n10 gutenberg1.txt010      42                30 FALSE          &lt;chr [0]&gt;  \n# ℹ 1 more variable: after_word_my_regexp &lt;chr&gt;\n\n\nThe above tibble is a sliced set of the poems dataframe, showing the first 10 poems after the text processing. Each row represents a single poem, identified by its poem ID. For the columns, poem_id is the unique identifier for each poem, n_lines the total number of lines in that poem, punctuation_count is the total number of punctuation marks in the poem (Dickinson used many punctuations in writing, especially dashes– I didn’t know how to specify the em-dash in r so I just did all punctuation), mentions_death is a Boolean that returns whether or not the poem contains death-related words (which I interpreted in death_regexp), years_found is a list containing any four-digit year-like patterns in the poem, and after_word_my_rgexp is the first word that follows “poem” in each poem, if present (using look behind, as Dickinson’s poems were often personal and invoked a lot of first-person pronouns)."
  },
  {
    "objectID": "Dickinson.html#plot-1-most-frequent-words",
    "href": "Dickinson.html#plot-1-most-frequent-words",
    "title": "Emily Dickinson Poems",
    "section": "Plot 1 – Most Frequent Words",
    "text": "Plot 1 – Most Frequent Words\n\nmost_used_words &lt;- tokenize |&gt;\n  count(words, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\nmost_used_words |&gt;\n  ggplot(aes(x = reorder(words, n), y = n)) +\n  geom_col(fill = \"cornflowerblue\") +\n  geom_text(aes(label = n),\n            size = 3) +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Most Frequent Words in Emily Dickinson’s Poems\",\n    x = \"Word\",\n    y = \"Number of Times Used\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThe above plot is a horizontal bar graph that shows the top 10 most frequent words in Emily Dickinson’s Poems. “The” is the most frequent word, with 1878 uses, followed by “a” and “and” with 775 and 767 uses respectively. Next, “to” is listed at 656 observations. It’s interesting to note that the next most used word, “I”, is a pronoun, and the only pronoun on the top 10. With 615 uses, it shows that most of Dickinson’s poems are written from first-person perspective or express personal thought and feeling. The rest of the top 10 is “of”, “it”, “in”, “that”, and “is”, with 543, 371, 357, 354, and 329 uses. The list reflects an analysis made by the Oxford English Corpus of the 100 most common words in written English, with 8 of the 10 of Dickinson’s most used words (only is and it are not in the top 10) being in the top 10, and only 1 (is) not being in the top 100. Dickinson, despite her unconventional punctuation and themes, still drew from everyday common English. Her uniqueness and innovation is not in her word choice, but rather her arrangement of words to create a complex story– a poem. Her poetry is accessible, as it uses familiar words, yet is profoundly layered as she explores her own emotions, thoughts, and existential reflections."
  },
  {
    "objectID": "Dickinson.html#poem-length-vs.-punctuation-count",
    "href": "Dickinson.html#poem-length-vs.-punctuation-count",
    "title": "Emily Dickinson Poems",
    "section": "Poem Length vs. Punctuation Count",
    "text": "Poem Length vs. Punctuation Count\n\nggplot(poems, aes(x = n_lines, y = punctuation_count)) +\n  geom_point(alpha = 0.5, color = \"seagreen1\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Correlation Between Poem Length and Punctuation Count\",\n    x = \"Number of Lines in Poem\",\n    y = \"Frequency of Punctuation(s)\"\n  ) +\n  theme_light()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe above plot is scatter plot that displays the correlation between poem length and punctuation count. In theory, the more lines in a poem, the greater the frequency of punctuation. I decided to use a linear model (method = “lm”) as I believed that the number of lines in a poem should function as a direct explanation for the punctuation count in Dickinson’s poems. While the regression line shows a general upward trend, the data points reveal considerable variation, especially as poem length increases. Most frequently, the longer the poem, Dickinson will use more punctuation than the model predicts she would use. The variation could reflect her unconventional approach to grammar and poetic rhythm, which serve stylistically to contribute to the poem’s tone and emotion. Punctuation in a Dickinson work is not merely just a structural device, but a deliberate choice that reflects the aesthetic and elasticity of her poetry."
  },
  {
    "objectID": "Dickinson.html#narrative",
    "href": "Dickinson.html#narrative",
    "title": "Emily Dickinson Poems",
    "section": "Narrative",
    "text": "Narrative\nIn completing this project, I wanted to have a better understanding of Emily Dickinson’s poetic style through a computational lens. While her writing is intensely emotional and personal, known for its introspection and peculiar use of punctuation, it also displays a structural consistency with recurring themes. I wondered if these qualities could be reflected quantitatively through text analysis. By counting word frequency, I was able to explore Dickinson’s vocabulary– interestingly, the most common words in her poetry are simple, everyday English words like “the”, “a”, and “and”. It suggests to me that rather than complex diction, Dickinson’s poetry’s power comes from how she arranges her work to express profound meaning. The presence of “I” supports a common interpretation that her work is self-reflective and personal.\nThe regular expressions I designed further support my question. I tried to detect text from poems which reference words related to death, one of Dickinson’s most enduring themes, and extracted any year-like numbers that might situate her work temporally or support the theme of death and ending. By identifying words that followed “my”, I tried to trace personal/possessive language– all these textual patterns tell the story of how Emily Dickinson engaged with mortality, memory, limit, and subjectivity in her work."
  },
  {
    "objectID": "dvspot.html",
    "href": "dvspot.html",
    "title": "Spotify",
    "section": "",
    "text": "The Spotify dataset contains a file named spotify_songs.csv, and is a collection of thousands of songs with unique variables such as track_id, track_name, track_artist, and track_popularity, among others. One of the variables I found to be interesting and analyzeable was “danceability.” Recently, I’ve started going to some parties again, but the DJs have been playing pretty bad songs… I decided to find out the artists who produce the “best” party music using the data. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dvspot.html#introduction",
    "href": "dvspot.html#introduction",
    "title": "Spotify",
    "section": "",
    "text": "The Spotify dataset contains a file named spotify_songs.csv, and is a collection of thousands of songs with unique variables such as track_id, track_name, track_artist, and track_popularity, among others. One of the variables I found to be interesting and analyzeable was “danceability.” Recently, I’ve started going to some parties again, but the DJs have been playing pretty bad songs… I decided to find out the artists who produce the “best” party music using the data. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dvspot.html#code-and-graph",
    "href": "dvspot.html#code-and-graph",
    "title": "Spotify",
    "section": "Code and Graph",
    "text": "Code and Graph\n\n#Filtered by popular tracks with scores &gt;= 90\npopular_tracks &lt;- spotify |&gt;\n  filter(track_popularity &gt;= 90)\n\n# Top 10 artists by average danceability among popular tracks\ntop_danceable_popular &lt;- popular_tracks |&gt;\n  group_by(track_artist) |&gt;\n  summarize(avg_danceability = mean(danceability, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  arrange(desc(avg_danceability)) |&gt;\n  slice_head(n = 10)\n\n# Plot\nggplot(top_danceable_popular, aes(x = reorder(track_artist, avg_danceability),\n                                  y = avg_danceability)) +\n  geom_col(fill = \"darkseagreen2\") +\n  geom_text(aes(label = round(avg_danceability, 2)),\n            hjust = -.1,\n            size = 2) +\n  coord_flip() +\n  labs(title = \"Most Danceable Artists filtered by Popular Tracks &gt;=90 (Frat Party Final Bosses)\",\n       x = \"Artist\",\n       y = \"Average Danceability\") +\n  theme_light()+\n  theme(plot.title = element_text(size = 12),\n        axis.text.y = element_text(size = 8)\n        )"
  },
  {
    "objectID": "dvspot.html#analysis",
    "href": "dvspot.html#analysis",
    "title": "Spotify",
    "section": "Analysis",
    "text": "Analysis\nInfluenced by party music, I created a horizontal bar chart to show the top 10 artists with tracks that have the highest average danceability. A score of 0 would represent undanceable, and a score of 1 to be extremely danceable. I decided to sort songs by popularities of greater than or equal to 90, because the song has to be “relevant” enough for a college to know and be able to vibe/dance to. Roddy Rich tops the list with the highest average danceability of 0.90 –he’s followed by artists like Dimelo Flow, Lil Uzi Vert, Regard, Rauw Alejandro, and Lil Nas X, with their scores at around 0.90. The above artists are Hip/Hop creators, Reggaeton singers, and DJs, which make sense– the cultural background of the music they create has a strong history of dance and high energy levels. Next, Y2K scored 0.84, Ed Sheeran scored 0.82, Tones and I 0.82, and Nicky Jam 0.82. The results suggest that among mainstream artists, hip-hop, reggaeton, and pop crossovers tend to dominate in term of danceable tracks. It’s important to note that filtering an artist by their top hits only may not accurately represent their music’s danceability. For example, Ed Sheeran has a lot of indie music on top of his pop hits, however, as they aren’t as relevant, the popularity filter does not account for the danceability of those tracks, increasing his score. However, I believe that the parameters still give us a good way to analyze which artists we should be playing to dance. So, the next time I go to a party, or high-octane social event, I better hear one of these artists! Or, we’ll just get some 2010s house again…"
  },
  {
    "objectID": "dvspot.html#link-and-credits",
    "href": "dvspot.html#link-and-credits",
    "title": "Spotify",
    "section": "Link and Credits",
    "text": "Link and Credits\nThe data comes from Spotify via the spotifyr package, authored by Charlie Thompson, Josiah Parry, Donal Phipps, and Tom Wolff. The link to the dataset can be accessed here: TidyTuesday Spotify Dataset. From https://open.spotify.com/."
  },
  {
    "objectID": "p1.html",
    "href": "p1.html",
    "title": "Sample of Piano",
    "section": "",
    "text": "Here’s a short clip of me learning a song on the piano! Hidekazu Sakamoto’s コウを追いかけて.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Dating Data",
    "section": "",
    "text": "In 2015, data scientists Albert Y. Kim and Adriana Escobedo-Land released a dataset of 59,946 collected San Francisco OkCupid users from a period in the 2010s. Their goal was to provide a real example of a complex datasetfor undergraduat students in introductory statistics and data-science classes to practice modeling, visualization, and text analysis in R (Kim & Escobedo-Land, 2015). The dataset included demographic variables, such as age, gender, height, religion, and lifestyle habits, but also personal responses to 10 open-ended essay questions.\nHowever, in 2021, Tiffany Xiao and Yifan Ma published a letter to the Journal of Statistics and Data Science Education named “A Call for Review of ‘OkCupid Data for Introductory Statistics and Data Science Courses’ by Albert Y. Kim and Adriana Escobedo-Land.” They argued that the dataset violated key ethical principles surrounding consent and personal identification (Xiao & Ma, 2021). Although the researchers had legal permission from OK Cupid to use the data, users were not explicitly informed that their profiles could be made public for the use of research or classroom use– especially for non-users of the dating app. The tension between public data and privacy in life is an example of one of the largest ethical dilemmas in modern data science: just because something is public and accessible, does it mean we can freely use it without restriction? The issue echoes contemporary controversies like the latest Tik Tok trend, Cheater Buster, which from what I’ve gathered, is an AI site that scrapes names, phone numbers, and faces from dating apps and social media profiles to detect potential cheating/infidelity– which raises similar concerns about digital surveillance and privacy.\nToday, by request of the original authors (Kim and Escobedo-Land), the publication has been corrected to further remove potential identifiers of the OkCupid users whose data was used as examples or scraped, while retaining its educational use. Ages were randomized, the data file was renamed, and the open-ended user responses were shuffled around in the 2021 revised edition (Kim & Escobedo-Land, 2021)."
  },
  {
    "objectID": "project4.html#introduction-and-scenario",
    "href": "project4.html#introduction-and-scenario",
    "title": "Dating Data",
    "section": "",
    "text": "In 2015, data scientists Albert Y. Kim and Adriana Escobedo-Land released a dataset of 59,946 collected San Francisco OkCupid users from a period in the 2010s. Their goal was to provide a real example of a complex datasetfor undergraduat students in introductory statistics and data-science classes to practice modeling, visualization, and text analysis in R (Kim & Escobedo-Land, 2015). The dataset included demographic variables, such as age, gender, height, religion, and lifestyle habits, but also personal responses to 10 open-ended essay questions.\nHowever, in 2021, Tiffany Xiao and Yifan Ma published a letter to the Journal of Statistics and Data Science Education named “A Call for Review of ‘OkCupid Data for Introductory Statistics and Data Science Courses’ by Albert Y. Kim and Adriana Escobedo-Land.” They argued that the dataset violated key ethical principles surrounding consent and personal identification (Xiao & Ma, 2021). Although the researchers had legal permission from OK Cupid to use the data, users were not explicitly informed that their profiles could be made public for the use of research or classroom use– especially for non-users of the dating app. The tension between public data and privacy in life is an example of one of the largest ethical dilemmas in modern data science: just because something is public and accessible, does it mean we can freely use it without restriction? The issue echoes contemporary controversies like the latest Tik Tok trend, Cheater Buster, which from what I’ve gathered, is an AI site that scrapes names, phone numbers, and faces from dating apps and social media profiles to detect potential cheating/infidelity– which raises similar concerns about digital surveillance and privacy.\nToday, by request of the original authors (Kim and Escobedo-Land), the publication has been corrected to further remove potential identifiers of the OkCupid users whose data was used as examples or scraped, while retaining its educational use. Ages were randomized, the data file was renamed, and the open-ended user responses were shuffled around in the 2021 revised edition (Kim & Escobedo-Land, 2021)."
  },
  {
    "objectID": "project4.html#permission-and-consent-structure-q1",
    "href": "project4.html#permission-and-consent-structure-q1",
    "title": "Dating Data",
    "section": "Permission and Consent Structure (Q1)",
    "text": "Permission and Consent Structure (Q1)\nAlthough Kim and Escobedo-Land received explicit permission from OkCupid leadership to use a scraped dataset– permission that technically satisfied the company-to-researcher requirement (Kim & Escobedo-Land, 2015)– this does not resolve the central ethical issue: individual users never consented to having their personal essays, profile details, or demographic information repurposed for public datasets. Users reasonably expect their profiles to be viewed only within the dating platform, not downloaded, scraped, or redistributed for research or teaching.\nOkCupid’s Terms of Use reinforce this expectation by prohibiting the use of “any robot [or] crawler” to extract data (Sec. 2c) and by stating that Member Content “belongs to the user” and cannot be copied or redistributed outside the service (Sec. 3b). These clauses indicate that automated scraping directly contradicts what users agreed to (OkCupid, 2025). Thus, even with corporate permission, users’ actual consent was never obtained.\nXiao and Ma argue that this gap between platform approval and user awareness reflects a broader problem: outdated U.S. data laws make legality an unreliable guide for ethical practice (Xiao & Ma, 2021). They emphasize that variables such as essays, demographics, and temporal or geographic markers remain identifiable and require stronger anonymization and more controlled access. The same concerns apply to modern systems like Cheater Buster, which scrape faces and personal details without user consent despite platform prohibitions. Across both cases, the underlying issue is the absence of meaningful, informed permission for intimate data to be used beyond its intended dating context."
  },
  {
    "objectID": "project4.html#data-collection-and-ethics-q3",
    "href": "project4.html#data-collection-and-ethics-q3",
    "title": "Dating Data",
    "section": "Data Collection and Ethics (Q3)",
    "text": "Data Collection and Ethics (Q3)\nKim and Escobedo-Land provide a clear description of how the OK Cupid dataset was collected (Kim & Escobedo-Land, 2015). The data were obtained by scraping public-facing OK Cupid profiles using a Python script. The authors explain that the dataset consists of “the public profiles of 59,946 OkCupid users who were living within 25 miles of San Francisco, had active profiles during a period in the 2010s, were online in the previous year, and had at least one picture in their profile.” They further note that “any non-publicly facing information such as messaging was not accessible,” confirming that only content visible to any OK Cupid user– or anyone browsing publicly viewable portions of the platform– was collected. The scrape occurred over a four-day window and captured typical demographic information and lifestyle habits. Random noise was added to the age variable as a basic anonymization step.\nWhether the observations were collected ethically is more complex. Legally, the profiles were publicly viewable, and OK Cupid gave the authors permission to use the data. Ethically, however, users likely did not expect their dating profiles to be harvested, stored offline, and distributed publicly for teaching or research. Xiao and Ma argue that even when scraping is technically permissible, datasets like this require stronger anonymization because public visibility does not equal informed consent– especially given the intimate nature of dating-profile content (Xiao & Ma, 2021). Their concern is not that the authors acted maliciously, but that relying solely on “public availability” overlooks the privacy expectations of the people represented."
  },
  {
    "objectID": "project4.html#identifiable-data-and-anonymization-q5",
    "href": "project4.html#identifiable-data-and-anonymization-q5",
    "title": "Dating Data",
    "section": "Identifiable Data and Anonymization (Q5)",
    "text": "Identifiable Data and Anonymization (Q5)\nThe OK Cupid dataset is only partially anonymized, and several components remained potentially identifiable even after initial cleaning. Kim and Escobedo-Land removed usernames and added random noise to the age variable, but they retained free-text profile essays and a number of demographic and lifestyle variables (Kim & Escobedo-Land, 2015) . Xiao and Ma specifically note that the dataset “was found to contain identifiable information” prior to correction, and they highlight several categories of variables that increase the risk of identification: temporal information, geographic information, and rich textual content (Xiao & Ma, 2021).\nSome of the data is identifiable because free-response essays can contain unique phrases, personal details, or references that users voluntarily included in their profiles. Even without explicit names, writing style and self-descriptions can function as identifiers, especially when matched against still-public OK Cupid profiles. Xiao and Ma argue that certain variables– such as the time of data collection– had “identification power disproportionate to their value for research,” meaning they made re-identification easier without improving the dataset’s pedagogical usefulness (Xiao & Ma, 2021).\nOther variables, such as ethnicity, height, specific lifestyle combinations, and narrow geographic radius (within 25 miles of San Francisco), further increase the potential for triangulation with public profiles. Although these variables are not uniquely identifying on their own, together they create detailed user signatures that could allow motivated actors to match individuals to their online profiles.\nAs for whether the dataset is sufficiently anonymized or old enough to be free of ethical concerns, Xiao and Ma’s letter suggests the answer is no (Xiao & Ma, 2021). They emphasize that outdated data-privacy laws cannot be relied upon to protect user identity and that additional anonymization steps were needed before public release. Being older does not eliminate risk: user essays and profile characteristics remain linkable if the original OK Cupid profiles– or archived versions– are still accessible.\nAnonymity is not guaranteed in this dataset. Rich text data, combined with demographic variables and temporal markers, makes complete de-identification effectively impossible. Xiao and Ma recommend alternative approaches such as removing certain variables altogether or distributing the data through secure channels instead of posting it publicly, underscoring that meaningful anonymity could not be assured with the dataset in its original form (Xiao & Ma, 2021)."
  },
  {
    "objectID": "project4.html#unintended-uses-q8",
    "href": "project4.html#unintended-uses-q8",
    "title": "Dating Data",
    "section": "Unintended Uses (Q8)",
    "text": "Unintended Uses (Q8)\nThere is evidence that OK Cupid data has been used in ways far beyond what Kim and Escobedo-Land originally intended (Kim & Escobedo-Land, 2015). Their stated purpose was explicitly pedagogical: the dataset was created for instructional use in introductory statistics and data-science courses. However, Xiao and Ma contextualize the dataset within a broader pattern of unintended and controversial reuse of OK Cupid data over the past decade, citing two specific incidents (Xiao & Ma, 2021).\nFirst, the 2014 case where an individual scraped OK Cupid profiles to analyze and optimize his dating strategy. The legality of his scraping was debated, raising questions about potential violations of the Computer Fraud and Abuse Act (Penenberg, 2014).\nSecond, the 2016 incident where researchers scraped and publicly posted data from 70,000 OK Cupid users without anonymization, causing widespread public outcry (Hackett, 2016). The researchers argued that their actions were legal because the profiles were publicly visible, but their release was widely condemned as an invasion of privacy.\nXiao and Ma use these examples to show that once dating-profile data is publicly released or publicly accessible, it is highly susceptible to re-purposing beyond its original educational context, regardless of intent (Xiao & Ma, 2021). Their letter makes it clear that relying on legality or “public availability” does not protect individuals from later misuse. Although Kim and Escobedo-Land did not encourage disruptive reuse, publicly releasing the dataset created the possibility of the same kind of function creep that occurred in 2014 and 2016. This includes the potential for re-identification, surveillance, analytical uses unrelated to teaching, or aggregation with other datasets. Xiao and Ma’s recommendation to remove certain variables or distribute the data through secure channels reflects their concern that once data is public, its uses can no longer be controlled. In this way, the OK Cupid dataset fits a broader pattern– dating-app data often ends up being used in ways far beyond what data subjects, or even researchers, originally envisioned."
  },
  {
    "objectID": "project4.html#why-care",
    "href": "project4.html#why-care",
    "title": "Dating Data",
    "section": "Why Care?",
    "text": "Why Care?\nThe issues raised in the OkCupid dataset and in modern scraping tools like Cheater Buster matter because they reveal a structural imbalance in who benefits from data extraction and who bears the risks. Researchers and developers gain access to detailed datasets that can be used to train AI, improve products, or for academic purposes. However, the users whose data makes this possible does not benefit, and often doesn’t even have the knowledge that their information, sometimes intimate, has been re-purposed, or shared around. This could cause reputational harm (imagine a fake tinder profile someone made of you and your insecure S.O. found you on Cheater Buster), or the loss of control over personal narratives that they originally shared only for dating purposes.\nThose most harmed tend to be individuals who already experience disproportionate risks online: LGBTQ+ users, people of color, and anyone who shares sensitive information in essays or photographs. These groups may be more easily identifiable due to facial features, mannerisms, or writing style, and the public release of such data heightens the risk of doxxing, harassment, or unwanted exposure. Meanwhile, the ones who scrape or redistribute their data face little to no consequences, in part because outdated U.S. privacy laws do not clearly regulate the repurposing of publicly viewable online information (Xiao & Ma, 2021).\nThe motivations behind such ethical violations often combine multiple forces. In the OkCupid case, the release was not driven by direct profit, but by academic convenience and the desire for a compelling dataset. In contrast, tools like Cheater Buster operate within an economy of surveillance– users can stalk others under the guise of transparency, accountability, or relationship protection. Whether for research credibility, product differentiation, or monetizing suspicion, these systems leverage personal data in ways that prioritize commercial benefit over individual privacy. Platforms and data collectors possess the technological and legal capacity to extract and repurpose user data, while individuals have limited awareness and even less control. Ensuring meaningful consent, limiting identifiability, and preventing unintended uses are essential steps toward restoring that balance and protecting demographics most affected by these decisions."
  },
  {
    "objectID": "project4.html#references",
    "href": "project4.html#references",
    "title": "Dating Data",
    "section": "References",
    "text": "References\nHackett, R. (2016). Researchers caused an uproar by publishing data from 70,000 okcupid users. Fortune.\nKim, A. Y., & Escobedo-Land, A. (2015). OkCupid data for introductory statistics and data science courses. Journal of Statistics Education, 23(2).\nPenenberg, A. L. (2014). Did the mathematician who hacked OkCupid violate federal computer laws?. Pando Daily. URL: http://pando. com/2014/01/22/did-the-mathematician-who-hacked-okcupid-violate-federalcomputer-laws/.\nTerms of Service. (2025). Retrieved November 11, 2025, from OkCupid, OkCupid Terms & Conditions Website, https://okcupid-app.zendesk.com/hc/en-us/articles/23941864418203-Terms-Conditions.\nXiao, T., & Ma, Y. (2021). A letter to the Journal of Statistics and Data Science Education– a call for review of “OkCupid data for introductory statistics and data science courses” by Albert Y. Kim and Adriana Escobedo-Land. Journal of Statistics and Data Science Education, 29(2), 214-215."
  }
]