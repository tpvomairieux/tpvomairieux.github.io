[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Phu’s Site for DS002",
    "section": "",
    "text": "Pomona College | Claremont, CA\nB. A in Computer Science | Aug. 2024 - May 2028"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Phu’s Site for DS002",
    "section": "",
    "text": "Pomona College | Claremont, CA\nB. A in Computer Science | Aug. 2024 - May 2028"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Phu’s Site for DS002",
    "section": "About Me",
    "text": "About Me\nHi! I’m Phu, a Sophmore at Pomona College. I’m originally from Nha Trang, Vietnam, but live in the states now (Richmond, VA). I can play guitar, piano, and ukelele, and am learning how to sing! If you want to see what i’m listening to currently, you can find my Spotify linked below my profile picture. I hope we can bond over music! Hope you enjoy my short page!"
  },
  {
    "objectID": "dveu.html",
    "href": "dveu.html",
    "title": "Eurovision Song Contest",
    "section": "",
    "text": "The Eurovision dataset contains a filed called eurovision.csv, and is a collection of data from Eurovision contests since its inception in 1956, with the exception of the 2020 Covid Year. The dataset represents information for all contestants for each year and by round, but it is important to note that changes in the scoring system have occurred throughout different points in time.For example, semi-final rounds were not introduced until 2005. Variables include the event, the host city, the artist country of origin, their song, the points they accumulated, and their rank, among others. I listened to a lot of Eurovision classics without having known before, so it was a pleasant surprise to do an analysis of the top Eurovision countries. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dveu.html#introduction",
    "href": "dveu.html#introduction",
    "title": "Eurovision Song Contest",
    "section": "",
    "text": "The Eurovision dataset contains a filed called eurovision.csv, and is a collection of data from Eurovision contests since its inception in 1956, with the exception of the 2020 Covid Year. The dataset represents information for all contestants for each year and by round, but it is important to note that changes in the scoring system have occurred throughout different points in time.For example, semi-final rounds were not introduced until 2005. Variables include the event, the host city, the artist country of origin, their song, the points they accumulated, and their rank, among others. I listened to a lot of Eurovision classics without having known before, so it was a pleasant surprise to do an analysis of the top Eurovision countries. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dveu.html#code-and-graph",
    "href": "dveu.html#code-and-graph",
    "title": "Eurovision Song Contest",
    "section": "Code and Graph",
    "text": "Code and Graph\n\n# Top 10 countries by cumulative total points since 2005\ntop_countries &lt;- euro |&gt;\n  filter(year &gt;= 2005) |&gt;\n  group_by(artist_country) |&gt;\n  summarize(sum_total_points = sum(total_points, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(sum_total_points)) |&gt;\n  slice_head(n = 10) # Cuts the list into just the top 10 countries.\n\n# Plot\nggplot(top_countries, aes(x = reorder(artist_country, sum_total_points),\n                          y = sum_total_points)) +\n  geom_col(fill = \"cornflowerblue\") + \n  geom_text(aes(label = sum_total_points),\n            hjust = -.1,\n            size = 2) +\n  coord_flip() +\n  labs(title = \"Top 10 Eurovision Countries by Total Points (2005–Present)\",\n       x = \"Country\",\n       y = \"Total Points\") +\n  theme_light()"
  },
  {
    "objectID": "dveu.html#analysis",
    "href": "dveu.html#analysis",
    "title": "Eurovision Song Contest",
    "section": "Analysis",
    "text": "Analysis\nThe horizontal bar chart shows the total Eurovision points earned by countries since 2005 (the introduction of the semi-final system, to make results more comparable to the modern contest). Sweden dominates the charts with 5669 points, followed by Ukraine with 4998 and Russia with 3977. Norway, Greece, Azerbaijan, and Moldova show solid performances in their Eurovision entries. In particular, Azerbaijan only entered the competition in 2008, so 3 years late, but is 6th in total accumulated points, with 3528– an extraordinary result.The following countries are followed by Serbia, then the Netherlands, and lastly Bulgaria, with 2945 points. Eastern European countries dominate the charts, which is surprising, given that the Eurovision “Big-Five,” all Western European countries (U.K., Spain, Germany, France, Italy) are now automatically qualified to the finals despite not scoring the highest. Perhaps, it’s time for a new “Big Five?” Or, do the countries not try as hard to score points, knowing that they are guaranteed finalists due to their financial backing of the competition? It is important to acknowledge that there have been some disbanded countries (countries that no longer exist, such as Serbia and Montenegro), or inactive countries, which affect the total accumulated score of the country (especially in a split like Serbia and Montenegro). I decided that it was better to still continue with total accumulated points compared to average points, because Serbia and Montenegro for example participated in a single contest, placed 2nd, and while is obviously an outlier, “rules” the dataset as the best country by average points."
  },
  {
    "objectID": "dveu.html#link-and-credits",
    "href": "dveu.html#link-and-credits",
    "title": "Eurovision Song Contest",
    "section": "Link and Credits",
    "text": "Link and Credits\nThe data is provided by Eurovision, and Tanya Shapiro and Bob Rudis shared methods to clean and scrape the data. The link to the dataset can be accessed here: TidyTuesday Eurovision Dataset. From https://eurovision.tv/."
  },
  {
    "objectID": "AnimalCrossing.html",
    "href": "AnimalCrossing.html",
    "title": "Animal Crossing",
    "section": "",
    "text": "For this week’s project, I decided to use the Animal Crossing: New Horizons Villager dataset from TinyTuesday, which was originally compiled from VillagerDB. In my analysis, I asked whether or not a villager’s birth half of the year (born between Jan. - June vs. born between July-Dec.) is associated with their personality– more specifically, if they are cranky or not. My hypothesis question would be, are villagers born in the first half of the year more or less likely to be cranky than those who are born later. I wanted to do this experiment because I realized most of my friends are born in the 2nd half of the year, more specifically Sept.-Nov., and I’m one of the only people born in the first half for some odd reason (Jan.). I thought it would be a great way to ragebait them and prove they are “just like the data” if I’m able to prove that people who are born in the latter half of the year are indeed more likely to be cranky than people born in the first half of the year."
  },
  {
    "objectID": "AnimalCrossing.html#introduction",
    "href": "AnimalCrossing.html#introduction",
    "title": "Animal Crossing",
    "section": "",
    "text": "For this week’s project, I decided to use the Animal Crossing: New Horizons Villager dataset from TinyTuesday, which was originally compiled from VillagerDB. In my analysis, I asked whether or not a villager’s birth half of the year (born between Jan. - June vs. born between July-Dec.) is associated with their personality– more specifically, if they are cranky or not. My hypothesis question would be, are villagers born in the first half of the year more or less likely to be cranky than those who are born later. I wanted to do this experiment because I realized most of my friends are born in the 2nd half of the year, more specifically Sept.-Nov., and I’m one of the only people born in the first half for some odd reason (Jan.). I thought it would be a great way to ragebait them and prove they are “just like the data” if I’m able to prove that people who are born in the latter half of the year are indeed more likely to be cranky than people born in the first half of the year."
  },
  {
    "objectID": "AnimalCrossing.html#simulation-details-what-i-plan-to-do",
    "href": "AnimalCrossing.html#simulation-details-what-i-plan-to-do",
    "title": "Animal Crossing",
    "section": "Simulation Details (What I plan to do)",
    "text": "Simulation Details (What I plan to do)\nTo test this, I conduct a permutation test simulating the null hypothesis (Ho) that there is no relationship between birth half and personality. My alternative hypothesis (Ha) says that people born in the latter half of the year, from July-Dec., are more likely to be cranky when compared to the first half of the year people. To derive this, I’ll first use lubridate to derive each villager’s birth month from the birthday variable and categorize them into Jan.-June or July-Dec. Then, I’ll calculate the observed differences in proportions of Cranky villagers between the two halves of the year. I wrote a function to randomly permute birth-half labels, recompute the difference in proportions, and return the value. Then, I used map_dbl() to repeat the process 1,000 times to generate a null-distribution of differences under random label assignment to estimate an empirical p-value by comparing the observed difference to the distribution of permuted differences. The first graph I’ll create shows the proportion of each personality across the two birth halves, to help visualize the observed data. The second graph is a histogram of the permutation distribution which shows what kind of proportion differences that should be expected if there was no true relationship between birth-half and crankiness."
  },
  {
    "objectID": "AnimalCrossing.html#data-source",
    "href": "AnimalCrossing.html#data-source",
    "title": "Animal Crossing",
    "section": "Data Source",
    "text": "Data Source\nThe data for the information about villagers comes from the TidyTuesday Animal Crossing: New Horizon dataset, available through the TidyTuesday Github Repository:Github. The information comes from Villager DB, which is a community-maintained database that compiles detailed profiles of all Animal Crossing villagers. I am currently unable to access this datasource or locate the individuals that put the data into a presentable form. Both the website VillagerDB and the Github repository are down, and after a quick Reddit search, I learned that they’ve been taken down recently, and I don’t know when it is coming back. For the original data in the game, the data was created by the developers, Nintendo, and the producer who “created” the Animal Crossing games would be Hisashi Nogami. More details about Animal Crossing and characters can be officially sourced to their website."
  },
  {
    "objectID": "AnimalCrossing.html#code",
    "href": "AnimalCrossing.html#code",
    "title": "Animal Crossing",
    "section": "Code",
    "text": "Code\n\nobserved_proportion_diff &lt;- function(target_personality = \"Cranky\") {\n  first_half &lt;- villagers |&gt;\n    filter(birth_half == \"Jan-Jun\", personality == target_personality) |&gt;\n    nrow() /\n    nrow(villagers |&gt; filter(birth_half == \"Jan-Jun\"))\n\n  \n  latter_half &lt;- villagers |&gt;\n    filter(birth_half == \"Jul-Dec\", personality == target_personality) |&gt;\n    nrow() /\n    nrow(villagers |&gt; filter(birth_half == \"Jul-Dec\"))\n\n  first_half - latter_half\n}\n\n\nobserved_proportion_diff(\"Cranky\")\n\n[1] -0.07486631\n\n\n\none_permutation &lt;- function(villagers, target_personality = \"Cranky\") {\n  villagers_permutation &lt;- villagers |&gt;\n    filter(!is.na(birth_half)) |&gt;\n    mutate(birth_half = sample(birth_half))\n  \n  \nfirst_half &lt;- villagers_permutation |&gt;\n  filter(birth_half == \"Jan-Jun\", personality == target_personality) |&gt;\n  nrow() /\n  (villagers_permutation |&gt;\n      filter(birth_half == \"Jan-Jun\") |&gt;\n      nrow())\n\n\nlatter_half &lt;- villagers_permutation |&gt;\n  filter(birth_half == \"Jul-Dec\", personality == target_personality) |&gt;\n  nrow() /\n  (villagers_permutation |&gt;\n      filter(birth_half == \"Jul-Dec\") |&gt;\n      nrow())\n  \n\nfirst_half - latter_half\n}\n\nset.seed(47)\n\npermutation_diff &lt;- map_dbl(1:1000, ~ one_permutation(villagers, \"Cranky\"))\n\nobserved_diff &lt;- observed_proportion_diff(\"Cranky\")\n\np_value &lt;- mean(abs(permutation_diff) &gt;= abs(observed_diff))\np_value\n\n[1] 0.039\n\n\n\ncranky_proportion &lt;- villagers |&gt;\n  filter(!is.na(birth_half)) |&gt;\n  group_by(birth_half) |&gt;\n  summarize(\n    cranky_prop = mean(personality == \"Cranky\")\n  )\n\ncranky_proportion |&gt;\n  ggplot(aes(x = birth_half, y = cranky_prop, fill = birth_half)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Proportion of Cranky Villagers by Birth Half\",\n    x = \"Birth Half of the Year\",\n    y = \"Proportion Cranky\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis bar chart shows the observed proportions of crankiness into two bins (Jan.-June births and July-Dec. births), which showed a difference of around 7.5 percentage points in proportion of crankiness in the villagers. Villagers born in the first half of the year have about 10% of their population having a cranky personality, while in the latter half of the year this proportion is around 17.5%.\n\ntibble(diff = permutation_diff) |&gt;\n  ggplot(aes(x = diff)) +\n  geom_histogram(bins = 30, fill = \"plum1\", color = \"black\") +\n  geom_vline(xintercept = observed_diff, color = \"red4\", linewidth = 1.0) +\n  labs(\n    title = \"Cranky Proportion Differences by Birth Half\",\n    x = \"Permuted difference (Jan-Jun - Jul-Dec)\",\n    y = \"Count\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis histogram displays the permutation distribution of proportion differences under the null hypothesis, with the vertical line marking the observed difference. The plot shows that the observed value lies at the edge of the simulated distribution, rather than at a 0.00 permuted difference, which provides visual evidence against the null hypothesis."
  },
  {
    "objectID": "AnimalCrossing.html#simulation-details-cont.-what-i-did-results-summary",
    "href": "AnimalCrossing.html#simulation-details-cont.-what-i-did-results-summary",
    "title": "Animal Crossing",
    "section": "Simulation Details (cont.) (What I did, Results Summary)",
    "text": "Simulation Details (cont.) (What I did, Results Summary)\nIn this simulation, I performed a permutation test to evaluate whether villagers born in the first half of the year (January–June) were more or less likely to be cranky than those born in the latter half (July–December). After categorizing villagers by their birth month and calculating the observed difference in proportions, I found that the proportion of cranky villagers was higher in the July–December group. The observed difference was approximately −0.0749, meaning the first-half group had about roughly 7.49% fewer cranky villagers on average. To test whether this difference could have arisen by chance, I simulated the null hypothesis by randomly shuffling the birth-half labels 1,000 times using my function one_permutation(), applied through map_dbl(). This generated a null distribution of differences assuming no true association between birth month and personality. The observed difference fell in the tail of this distribution, corresponding to an empirical p-value of approximately 0.039, and as 0.039 &lt; 0.05, it suggests the result is statistically significant at the 95% confidence level.\nOverall, the simulation supports the hypothesis that villagers born in the latter half of the year are slightly more likely to be cranky– we should reject Ho, as villagers born from July onwards are more likely to be cranky compared to their earlier born counterparts. Permutation testing can be used to assess non-parametric relationships between categorical variables, and in this case, it appears that people born July and afterwards are indeed more upset than those born from January thru. June ;)."
  },
  {
    "objectID": "Dickinson.html",
    "href": "Dickinson.html",
    "title": "Emily Dickinson Poems",
    "section": "",
    "text": "For this week’s project, I decided to do a full-text analysis from a collection of all of Emily Dickinson’s poems, transcribed by Jim Tinsley and made freely available on the Gutenberg project. In my analysis, I use 6 str_*() functions, those being str_count(), str_detect(), str_to_lower(), str_split(), str_extract(), and str_extract_all(). The lookaround I use is a lookbehind. I use 4 regular expressions – 1) to detect whether or not a poem contains death related words, 2) to extract any “year-like” number (4-digits), 3) during the lookbehind, and 4) to count the number of punctuation Dickinson uses. My two plots include one that graphs the top 10 most frequent words in Dickinson poems, and the second being an analysis of poem length to punctuation count– which, we should see, that the longer the poem length, the more punctuation it features."
  },
  {
    "objectID": "Dickinson.html#introduction",
    "href": "Dickinson.html#introduction",
    "title": "Emily Dickinson Poems",
    "section": "",
    "text": "For this week’s project, I decided to do a full-text analysis from a collection of all of Emily Dickinson’s poems, transcribed by Jim Tinsley and made freely available on the Gutenberg project. In my analysis, I use 6 str_*() functions, those being str_count(), str_detect(), str_to_lower(), str_split(), str_extract(), and str_extract_all(). The lookaround I use is a lookbehind. I use 4 regular expressions – 1) to detect whether or not a poem contains death related words, 2) to extract any “year-like” number (4-digits), 3) during the lookbehind, and 4) to count the number of punctuation Dickinson uses. My two plots include one that graphs the top 10 most frequent words in Dickinson poems, and the second being an analysis of poem length to punctuation count– which, we should see, that the longer the poem length, the more punctuation it features."
  },
  {
    "objectID": "Dickinson.html#data-source",
    "href": "Dickinson.html#data-source",
    "title": "Emily Dickinson Poems",
    "section": "Data Source",
    "text": "Data Source\nThe data of the Emily Dickinson Poems come from the Amherst-Statistics/DickinsonPoems GitHub repository, created by Nicholas Horton (reachable at nicholasjhorton@gmail.com), which includes transcriptions of Dickinson’s poems in the DickinsonPoems package. The link to the GitHub repo can be accessed via the following: Github.The dataset comes from Project Gutenberg, which is an e-Library of over 75,000 eBooks. The author of the poems, is, well, Emily Dickinson, and the transcriptions were produced by Jim Tinsley, who can be contacted at jtinsley@pobox.com. These poems can be accesed here: Gutenberg."
  },
  {
    "objectID": "Dickinson.html#code",
    "href": "Dickinson.html#code",
    "title": "Emily Dickinson Poems",
    "section": "Code",
    "text": "Code\n\n# Regular Expressions\n\ndeath_regexp &lt;- \"\\\\b(death|die|grave|funeral|corpse|end|demise|passing)\\\\b\"\n\nyear_regexp &lt;- \"\\\\b(10|11|12|13|14|15|16|17|18|19|20)\\\\d{2}\\\\b\"\n\n# LookAround (Look Behind)\n\nafter_word_my_regexp &lt;- \"(?&lt;=\\\\bmy\\\\s)[A-Za-z']+\"\n\npoems &lt;- poems |&gt;\n  mutate(\n    \n    punctuation_count = str_count(text, \"[[:punct:]]\"),\n    mentions_death = str_detect(str_to_lower(text), death_regexp),\n    years_found = str_extract_all(text, year_regexp),\n    after_word_my_regexp = str_extract(text, after_word_my_regexp)\n          \n  )\n\npoems |&gt; select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |&gt; head(10)\n\n# A tibble: 10 × 6\n   poem_id           n_lines punctuation_count mentions_death years_found\n   &lt;chr&gt;               &lt;int&gt;             &lt;int&gt; &lt;lgl&gt;          &lt;list&gt;     \n 1 gutenberg1.txt001      26                26 FALSE          &lt;chr [0]&gt;  \n 2 gutenberg1.txt002      15                13 FALSE          &lt;chr [0]&gt;  \n 3 gutenberg1.txt003      17                10 FALSE          &lt;chr [0]&gt;  \n 4 gutenberg1.txt004      28                35 TRUE           &lt;chr [0]&gt;  \n 5 gutenberg1.txt005      25                24 FALSE          &lt;chr [0]&gt;  \n 6 gutenberg1.txt006      13                 7 FALSE          &lt;chr [0]&gt;  \n 7 gutenberg1.txt007      17                10 FALSE          &lt;chr [0]&gt;  \n 8 gutenberg1.txt008      20                16 TRUE           &lt;chr [0]&gt;  \n 9 gutenberg1.txt009      15                11 TRUE           &lt;chr [0]&gt;  \n10 gutenberg1.txt010      42                30 FALSE          &lt;chr [0]&gt;  \n# ℹ 1 more variable: after_word_my_regexp &lt;chr&gt;\n\n\nThe above tibble is a sliced set of the poems dataframe, showing the first 10 poems after the text processing. Each row represents a single poem, identified by its poem ID. For the columns, poem_id is the unique identifier for each poem, n_lines the total number of lines in that poem, punctuation_count is the total number of punctuation marks in the poem (Dickinson used many punctuations in writing, especially dashes– I didn’t know how to specify the em-dash in r so I just did all punctuation), mentions_death is a Boolean that returns whether or not the poem contains death-related words (which I interpreted in death_regexp), years_found is a list containing any four-digit year-like patterns in the poem, and after_word_my_rgexp is the first word that follows “poem” in each poem, if present (using look behind, as Dickinson’s poems were often personal and invoked a lot of first-person pronouns)."
  },
  {
    "objectID": "Dickinson.html#plot-1-most-frequent-words",
    "href": "Dickinson.html#plot-1-most-frequent-words",
    "title": "Emily Dickinson Poems",
    "section": "Plot 1 – Most Frequent Words",
    "text": "Plot 1 – Most Frequent Words\n\nmost_used_words &lt;- tokenize_filtered |&gt;\n  count(words, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\nmost_used_words |&gt;\n  ggplot(aes(x = reorder(words, n), y = n)) +\n  geom_col(fill = \"cornflowerblue\") +\n  geom_text(aes(label = n),\n            size = 3) +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Most Frequent Words in Emily Dickinson’s Poems\",\n    x = \"Word\",\n    y = \"Number of Times Used\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThe above bar chart displays the top 10 most frequent meaningful words in Emily Dickinson’s poems after removing common stop words and filtering out fragmented tokens produced by contractions. A lot of common structural and grammatical words were filtered out because I wanted to focus on words that carry semantic weight to reveal the core ideas shaping Dickinson’s poetry. The most frequent word is “little,” with 91 uses, which may reflect Dickinson’s fascination with measuring life in small units, from tiny distances, brief encounters, to implied emotions. Her attention to miniature detail often elevates the ordinary into the spiritual or metaphysical. In this way, little becomes an aesthetic strategy as much as a descriptor. Words such as “day,” “life,” and “till” reflect her preoccupation with boundaries and transitions– the movement between life and death, morning and night, presence and absence. The repeated appearance of “will” and “till” suggests an emotional grammar built around uncertainty, anticipation, and incompletion, which is further backed by the word “away,” which shows seperation and space."
  },
  {
    "objectID": "Dickinson.html#poem-length-vs.-punctuation-count",
    "href": "Dickinson.html#poem-length-vs.-punctuation-count",
    "title": "Emily Dickinson Poems",
    "section": "Poem Length vs. Punctuation Count",
    "text": "Poem Length vs. Punctuation Count\n\nggplot(poems, aes(x = n_lines, y = punctuation_count)) +\n  geom_point(alpha = 0.5, color = \"seagreen1\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Correlation Between Poem Length and Punctuation Count\",\n    x = \"Number of Lines in Poem\",\n    y = \"Frequency of Punctuation(s)\"\n  ) +\n  theme_light()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe above plot is scatter plot that displays the correlation between poem length and punctuation count. In theory, the more lines in a poem, the greater the frequency of punctuation. I decided to use a linear model (method = “lm”) as I believed that the number of lines in a poem should function as a direct explanation for the punctuation count in Dickinson’s poems. While the regression line shows a general upward trend, the data points reveal considerable variation, especially as poem length increases. Most frequently, the longer the poem, Dickinson will use more punctuation than the model predicts she would use. The variation could reflect her unconventional approach to grammar and poetic rhythm, which serve stylistically to contribute to the poem’s tone and emotion. Punctuation in a Dickinson work is not merely just a structural device, but a deliberate choice that reflects the aesthetic and elasticity of her poetry."
  },
  {
    "objectID": "Dickinson.html#narrative",
    "href": "Dickinson.html#narrative",
    "title": "Emily Dickinson Poems",
    "section": "Narrative",
    "text": "Narrative\nIn completing this project, I wanted to have a better understanding of Emily Dickinson’s poetic style through a computational lens. While her writing is intensely emotional and personal, known for its introspection and peculiar use of punctuation, it also displays a structural consistency with recurring themes. I wondered if these qualities could be reflected quantitatively through text analysis. By counting word frequency, I was able to explore Dickinson’s vocabulary– not merely through the most common grammatical words in English, but through a filtered set of meaningful poetic terms. Rather than being driven by elaborate word choice and diction, Dickinson repeatedly relies on small, expressive words such as little, day, will, and till, which reveal her focus on fine detail, fleeting experiences, and the suspension of action. Her most common words do not name large abstractions, but rather they gesture toward partial states and smallness, inviting interpretation through what is tentative, approximate, or unfinished. This pattern reinforces the sense that Dickinson’s poetic power lies not in elaborate language, but in her choices that make everyday words carry deeper philosophical imagery and weight.\nThe regular expressions I designed further support my question. I tried to detect text from poems which reference words related to death, one of Dickinson’s most enduring themes, and extracted any year-like numbers that might situate her work temporally or support the theme of death and ending. By identifying words that followed “my”, I tried to trace personal/possessive language– all these textual patterns tell the story of how Emily Dickinson engaged with mortality, memory, limit, and subjectivity in her work."
  },
  {
    "objectID": "dvspot.html",
    "href": "dvspot.html",
    "title": "Spotify",
    "section": "",
    "text": "The Spotify dataset contains a file named spotify_songs.csv, and is a collection of thousands of songs with unique variables such as track_id, track_name, track_artist, and track_popularity, among others. One of the variables I found to be interesting and analyzeable was “danceability.” Recently, I’ve started going to some parties again, but the DJs have been playing pretty bad songs… I decided to find out the artists who produce the “best” party music using the data. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dvspot.html#introduction",
    "href": "dvspot.html#introduction",
    "title": "Spotify",
    "section": "",
    "text": "The Spotify dataset contains a file named spotify_songs.csv, and is a collection of thousands of songs with unique variables such as track_id, track_name, track_artist, and track_popularity, among others. One of the variables I found to be interesting and analyzeable was “danceability.” Recently, I’ve started going to some parties again, but the DJs have been playing pretty bad songs… I decided to find out the artists who produce the “best” party music using the data. Sources of the TidyTuesday dataset will be provided at the bottom of the page."
  },
  {
    "objectID": "dvspot.html#code-and-graph",
    "href": "dvspot.html#code-and-graph",
    "title": "Spotify",
    "section": "Code and Graph",
    "text": "Code and Graph\n\n#Filtered by popular tracks with scores &gt;= 90\npopular_tracks &lt;- spotify |&gt;\n  filter(track_popularity &gt;= 90)\n\n# Top 10 artists by average danceability among popular tracks\ntop_danceable_popular &lt;- popular_tracks |&gt;\n  group_by(track_artist) |&gt;\n  summarize(avg_danceability = mean(danceability, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  arrange(desc(avg_danceability)) |&gt;\n  slice_head(n = 10)\n\n# Plot\nggplot(top_danceable_popular, aes(x = reorder(track_artist, avg_danceability),\n                                  y = avg_danceability)) +\n  geom_col(fill = \"darkseagreen2\") +\n  geom_text(aes(label = round(avg_danceability, 2)),\n            hjust = -.1,\n            size = 2) +\n  coord_flip() +\n  labs(title = \"Most Danceable Artists filtered by Popular Tracks &gt;=90 (Frat Party Final Bosses)\",\n       x = \"Artist\",\n       y = \"Average Danceability\") +\n  theme_light()+\n  theme(plot.title = element_text(size = 12),\n        axis.text.y = element_text(size = 8)\n        )"
  },
  {
    "objectID": "dvspot.html#analysis",
    "href": "dvspot.html#analysis",
    "title": "Spotify",
    "section": "Analysis",
    "text": "Analysis\nInfluenced by party music, I created a horizontal bar chart to show the top 10 artists with tracks that have the highest average danceability. A score of 0 would represent undanceable, and a score of 1 to be extremely danceable. I decided to sort songs by popularities of greater than or equal to 90, because the song has to be “relevant” enough for a college to know and be able to vibe/dance to. Roddy Rich tops the list with the highest average danceability of 0.90 –he’s followed by artists like Dimelo Flow, Lil Uzi Vert, Regard, Rauw Alejandro, and Lil Nas X, with their scores at around 0.90. The above artists are Hip/Hop creators, Reggaeton singers, and DJs, which make sense– the cultural background of the music they create has a strong history of dance and high energy levels. Next, Y2K scored 0.84, Ed Sheeran scored 0.82, Tones and I 0.82, and Nicky Jam 0.82. The results suggest that among mainstream artists, hip-hop, reggaeton, and pop crossovers tend to dominate in term of danceable tracks. It’s important to note that filtering an artist by their top hits only may not accurately represent their music’s danceability. For example, Ed Sheeran has a lot of indie music on top of his pop hits, however, as they aren’t as relevant, the popularity filter does not account for the danceability of those tracks, increasing his score. However, I believe that the parameters still give us a good way to analyze which artists we should be playing to dance. So, the next time I go to a party, or high-octane social event, I better hear one of these artists! Or, we’ll just get some 2010s house again…"
  },
  {
    "objectID": "dvspot.html#link-and-credits",
    "href": "dvspot.html#link-and-credits",
    "title": "Spotify",
    "section": "Link and Credits",
    "text": "Link and Credits\nThe data comes from Spotify via the spotifyr package, authored by Charlie Thompson, Josiah Parry, Donal Phipps, and Tom Wolff. The link to the dataset can be accessed here: TidyTuesday Spotify Dataset. From https://open.spotify.com/."
  },
  {
    "objectID": "p1.html",
    "href": "p1.html",
    "title": "Sample of Piano",
    "section": "",
    "text": "Here’s a short clip of me learning a song on the piano! Hidekazu Sakamoto’s コウを追いかけて.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Dating Data",
    "section": "",
    "text": "In 2015, data scientists Albert Y. Kim and Adriana Escobedo-Land released a dataset of 59,946 collected San Francisco OkCupid users from a period in the 2010s. Their goal was to provide a real example of a complex datasetfor undergraduat students in introductory statistics and data-science classes to practice modeling, visualization, and text analysis in R (Kim & Escobedo-Land, 2015). The dataset included demographic variables, such as age, gender, height, religion, and lifestyle habits, but also personal responses to 10 open-ended essay questions.\nHowever, in 2021, Tiffany Xiao and Yifan Ma published a letter to the Journal of Statistics and Data Science Education named “A Call for Review of ‘OkCupid Data for Introductory Statistics and Data Science Courses’ by Albert Y. Kim and Adriana Escobedo-Land.” They argued that the dataset violated key ethical principles surrounding consent and personal identification (Xiao & Ma, 2021). Although the researchers had legal permission from OK Cupid to use the data, users were not explicitly informed that their profiles could be made public for the use of research or classroom use– especially for non-users of the dating app. The tension between public data and privacy in life is an example of one of the largest ethical dilemmas in modern data science: just because something is public and accessible, does it mean we can freely use it without restriction? The issue echoes contemporary controversies like the latest Tik Tok trend, Cheater Buster, which from what I’ve gathered, is an AI site that scrapes names, phone numbers, and faces from dating apps and social media profiles to detect potential cheating/infidelity– which raises similar concerns about digital surveillance and privacy. Cheater Buster is basically a service that allows anyone to search dating platforms such as Tinder or Hinge by entering a name or phone number, or uploading an image to the site, to check whether or not a partner has an active profile. The tool scrapes personal data and photos from dating apps to flag “cheaters,” even though the individuals searched have not consented to being monitored in this way, raising similar concerns about digital surveillance and privacy.\nToday, by request of the original authors (Kim and Escobedo-Land), the publication has been corrected to further remove potential identifiers of the OkCupid users whose data was used as examples or scraped, while retaining its educational use. Ages were randomized, the data file was renamed, and the open-ended user responses were shuffled around in the 2021 revised edition (Kim & Escobedo-Land, 2021)."
  },
  {
    "objectID": "project4.html#introduction-and-scenario",
    "href": "project4.html#introduction-and-scenario",
    "title": "Dating Data",
    "section": "",
    "text": "In 2015, data scientists Albert Y. Kim and Adriana Escobedo-Land released a dataset of 59,946 collected San Francisco OkCupid users from a period in the 2010s. Their goal was to provide a real example of a complex datasetfor undergraduat students in introductory statistics and data-science classes to practice modeling, visualization, and text analysis in R (Kim & Escobedo-Land, 2015). The dataset included demographic variables, such as age, gender, height, religion, and lifestyle habits, but also personal responses to 10 open-ended essay questions.\nHowever, in 2021, Tiffany Xiao and Yifan Ma published a letter to the Journal of Statistics and Data Science Education named “A Call for Review of ‘OkCupid Data for Introductory Statistics and Data Science Courses’ by Albert Y. Kim and Adriana Escobedo-Land.” They argued that the dataset violated key ethical principles surrounding consent and personal identification (Xiao & Ma, 2021). Although the researchers had legal permission from OK Cupid to use the data, users were not explicitly informed that their profiles could be made public for the use of research or classroom use– especially for non-users of the dating app. The tension between public data and privacy in life is an example of one of the largest ethical dilemmas in modern data science: just because something is public and accessible, does it mean we can freely use it without restriction? The issue echoes contemporary controversies like the latest Tik Tok trend, Cheater Buster, which from what I’ve gathered, is an AI site that scrapes names, phone numbers, and faces from dating apps and social media profiles to detect potential cheating/infidelity– which raises similar concerns about digital surveillance and privacy. Cheater Buster is basically a service that allows anyone to search dating platforms such as Tinder or Hinge by entering a name or phone number, or uploading an image to the site, to check whether or not a partner has an active profile. The tool scrapes personal data and photos from dating apps to flag “cheaters,” even though the individuals searched have not consented to being monitored in this way, raising similar concerns about digital surveillance and privacy.\nToday, by request of the original authors (Kim and Escobedo-Land), the publication has been corrected to further remove potential identifiers of the OkCupid users whose data was used as examples or scraped, while retaining its educational use. Ages were randomized, the data file was renamed, and the open-ended user responses were shuffled around in the 2021 revised edition (Kim & Escobedo-Land, 2021)."
  },
  {
    "objectID": "project4.html#permission-and-consent-structure-q1",
    "href": "project4.html#permission-and-consent-structure-q1",
    "title": "Dating Data",
    "section": "Permission and Consent Structure (Q1)",
    "text": "Permission and Consent Structure (Q1)\nAlthough Kim and Escobedo-Land received explicit permission from OkCupid leadership to use a scraped dataset– permission that technically satisfied the company-to-researcher requirement (Kim & Escobedo-Land, 2015)– this does not resolve the central ethical issue: individual users never consented to having their personal essays, profile details, or demographic information repurposed for public datasets. Users reasonably expect their profiles to be viewed only within the dating platform, not downloaded, scraped, or redistributed for research or teaching. Basically, obtaining corporate permission satisfied legal requirements, but it bypassed the consent of the people whose intimate data was being studied, shared, and later publicly distributed.\nOkCupid’s Terms of Use reinforce this expectation by prohibiting the use of “any robot [or] crawler” to extract data (Sec. 2c) and by stating that Member Content “belongs to the user” and cannot be copied or redistributed outside the service (Sec. 3b). These clauses indicate that automated scraping directly contradicts what users agreed to (OkCupid, 2025). Thus, even with corporate permission, users’ actual consent was never obtained.\nXiao and Ma argue that this gap between platform approval and user awareness reflects a broader problem: outdated U.S. data laws make legality an unreliable guide for ethical practice (Xiao & Ma, 2021). They emphasize that variables such as essays, demographics, and temporal or geographic markers remain identifiable and require stronger anonymization and more controlled access. The same concerns apply to modern systems like Cheater Buster, which scrape faces and personal details without user consent despite platform prohibitions. Across both cases, the underlying issue is the absence of meaningful, informed permission for intimate data to be used beyond its intended dating context."
  },
  {
    "objectID": "project4.html#data-collection-and-ethics-q3",
    "href": "project4.html#data-collection-and-ethics-q3",
    "title": "Dating Data",
    "section": "Data Collection and Ethics (Q3)",
    "text": "Data Collection and Ethics (Q3)\nKim and Escobedo-Land provide a clear description of how the OK Cupid dataset was collected (Kim & Escobedo-Land, 2015). The data were obtained by scraping public-facing OK Cupid profiles using a Python script. The authors explain that the dataset consists of “the public profiles of 59,946 OkCupid users who were living within 25 miles of San Francisco, had active profiles during a period in the 2010s, were online in the previous year, and had at least one picture in their profile.” They further note that “any non-publicly facing information such as messaging was not accessible,” confirming that only content visible to any OK Cupid user– or anyone browsing publicly viewable portions of the platform– was collected. The scrape occurred over a four-day window and captured typical demographic information and lifestyle habits. Random noise was added to the age variable as a basic anonymization step.\nWhether the observations were collected ethically is more complex. Legally, the profiles were publicly viewable, and OK Cupid gave the authors permission to use the data. Ethically, however, users likely did not expect their dating profiles to be harvested, stored offline, and distributed publicly for teaching or research. Xiao and Ma argue that even when scraping is technically permissible, datasets like this require stronger anonymization because public visibility does not equal informed consent– especially given the intimate nature of dating-profile content (Xiao & Ma, 2021). Their concern is not that the authors acted maliciously, but that relying solely on “public availability” overlooks the privacy expectations of the people represented."
  },
  {
    "objectID": "project4.html#identifiable-data-and-anonymization-q5",
    "href": "project4.html#identifiable-data-and-anonymization-q5",
    "title": "Dating Data",
    "section": "Identifiable Data and Anonymization (Q5)",
    "text": "Identifiable Data and Anonymization (Q5)\nThe OK Cupid dataset is only partially anonymized, and several components remained potentially identifiable even after initial cleaning. Kim and Escobedo-Land removed usernames and added random noise to the age variable, but they retained free-text profile essays and a number of demographic and lifestyle variables (Kim & Escobedo-Land, 2015) . Xiao and Ma specifically note that the dataset “was found to contain identifiable information” prior to correction, and they highlight several categories of variables that increase the risk of identification: temporal information, geographic information, and rich textual content (Xiao & Ma, 2021).\nSome of the data is identifiable because free-response essays can contain unique phrases, personal details, or references that users voluntarily included in their profiles. Even without explicit names, writing style and self-descriptions can function as identifiers, especially when matched against still-public OK Cupid profiles. Xiao and Ma argue that certain variables– such as the time of data collection– had “identification power disproportionate to their value for research,” meaning they made re-identification easier without improving the dataset’s pedagogical usefulness (Xiao & Ma, 2021).\nOther variables, such as ethnicity, height, specific lifestyle combinations, and narrow geographic radius (within 25 miles of San Francisco), further increase the potential for triangulation with public profiles. Although these variables are not uniquely identifying on their own, together they create detailed user signatures that could allow motivated actors to match individuals to their online profiles.\nAs for whether the dataset is sufficiently anonymized or old enough to be free of ethical concerns, Xiao and Ma’s letter suggests the answer is no (Xiao & Ma, 2021). They emphasize that outdated data-privacy laws cannot be relied upon to protect user identity and that additional anonymization steps were needed before public release. Being older does not eliminate risk: user essays and profile characteristics remain linkable if the original OK Cupid profiles– or archived versions– are still accessible.\nAnonymity is not guaranteed in this dataset. Rich text data, combined with demographic variables and temporal markers, makes complete de-identification effectively impossible. Xiao and Ma recommend alternative approaches such as removing certain variables altogether or distributing the data through secure channels instead of posting it publicly, underscoring that meaningful anonymity could not be assured with the dataset in its original form (Xiao & Ma, 2021)."
  },
  {
    "objectID": "project4.html#unintended-uses-q8",
    "href": "project4.html#unintended-uses-q8",
    "title": "Dating Data",
    "section": "Unintended Uses (Q8)",
    "text": "Unintended Uses (Q8)\nThere is evidence that OK Cupid data has been used in ways far beyond what Kim and Escobedo-Land originally intended (Kim & Escobedo-Land, 2015). Their stated purpose was explicitly pedagogical: the dataset was created for instructional use in introductory statistics and data-science courses. However, Xiao and Ma contextualize the dataset within a broader pattern of unintended and controversial reuse of OK Cupid data over the past decade, citing two specific incidents (Xiao & Ma, 2021).\nFirst, the 2014 case where an individual scraped OK Cupid profiles to analyze and optimize his dating strategy. The legality of his scraping was debated, raising questions about potential violations of the Computer Fraud and Abuse Act (Penenberg, 2014).\nSecond, the 2016 incident where researchers scraped and publicly posted data from 70,000 OK Cupid users without anonymization, causing widespread public outcry (Hackett, 2016). The researchers argued that their actions were legal because the profiles were publicly visible, but their release was widely condemned as an invasion of privacy.\nXiao and Ma use these examples to show that once dating-profile data is publicly released or publicly accessible, it is highly susceptible to re-purposing beyond its original educational context, regardless of intent (Xiao & Ma, 2021). Their letter makes it clear that relying on legality or “public availability” does not protect individuals from later misuse. Although Kim and Escobedo-Land did not encourage disruptive reuse, publicly releasing the dataset created the possibility of the same kind of function creep that occurred in 2014 and 2016. This includes the potential for re-identification, surveillance, analytical uses unrelated to teaching, or aggregation with other datasets. Xiao and Ma’s recommendation to remove certain variables or distribute the data through secure channels reflects their concern that once data is public, its uses can no longer be controlled. In this way, the OK Cupid dataset fits a broader pattern– dating-app data often ends up being used in ways far beyond what data subjects, or even researchers, originally envisioned."
  },
  {
    "objectID": "project4.html#why-care",
    "href": "project4.html#why-care",
    "title": "Dating Data",
    "section": "Why Care?",
    "text": "Why Care?\nThe issues raised in the OkCupid dataset and in modern scraping tools like Cheater Buster matter because they reveal a structural imbalance in who benefits from data extraction and who bears the risks. Researchers and developers gain access to detailed datasets that can be used to train AI, improve products, or for academic purposes. However, the users whose data makes this possible does not benefit, and often doesn’t even have the knowledge that their information, sometimes intimate, has been re-purposed, or shared around. This could cause reputational harm (imagine a fake tinder profile someone made of you and your insecure S.O. found you on Cheater Buster), or the loss of control over personal narratives that they originally shared only for dating purposes.\nLike the OkCupid scrape, Cheater Buster reframes dating as a public data source, not a personal space. The service presents itself as a tool for honesty or relationship protection, yet it enables surveillance of unsuspecting individuals using data they never agreed to share outside the app. Those most harmed tend to be individuals who already experience disproportionate risks online: LGBTQ+ users, people of color, and anyone who shares sensitive information in essays or photographs. These groups may be more easily identifiable due to facial features, mannerisms, or writing style, and the public release of such data heightens the risk of doxxing, harassment, or unwanted exposure. Meanwhile, the ones who scrape or redistribute their data face little to no consequences, in part because outdated U.S. privacy laws do not clearly regulate the repurposing of publicly viewable online information (Xiao & Ma, 2021).\nThe motivations behind such ethical violations often combine multiple forces. In the OkCupid case, the release was not driven by direct profit, but by academic convenience and the desire for a compelling dataset. In contrast, tools like Cheater Buster operate within an economy of surveillance– users can stalk others under the guise of transparency, accountability, or relationship protection. Whether for research credibility, product differentiation, or monetizing suspicion, these systems leverage personal data in ways that prioritize commercial benefit over individual privacy. Platforms and data collectors possess the technological and legal capacity to extract and repurpose user data, while individuals have limited awareness and even less control. Ensuring meaningful consent, limiting identifiability, and preventing unintended uses are essential steps toward restoring that balance and protecting demographics most affected by these decisions."
  },
  {
    "objectID": "project4.html#references",
    "href": "project4.html#references",
    "title": "Dating Data",
    "section": "References",
    "text": "References\nHackett, R. (2016). Researchers caused an uproar by publishing data from 70,000 okcupid users. Fortune.\nKim, A. Y., & Escobedo-Land, A. (2015). OkCupid data for introductory statistics and data science courses. Journal of Statistics Education, 23(2).\nPenenberg, A. L. (2014). Did the mathematician who hacked OkCupid violate federal computer laws?. Pando Daily. URL: http://pando. com/2014/01/22/did-the-mathematician-who-hacked-okcupid-violate-federalcomputer-laws/.\nTerms of Service. (2025). Retrieved November 11, 2025, from OkCupid, OkCupid Terms & Conditions Website, https://okcupid-app.zendesk.com/hc/en-us/articles/23941864418203-Terms-Conditions.\nXiao, T., & Ma, Y. (2021). A letter to the Journal of Statistics and Data Science Education– a call for review of “OkCupid data for introductory statistics and data science courses” by Albert Y. Kim and Adriana Escobedo-Land. Journal of Statistics and Data Science Education, 29(2), 214-215."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Racial Differences in Traffic-Stop Search Rates across Nebraska, Masaschusetts, and Virginia",
    "section": "",
    "text": "DESCRIBE ne_statewide_2020_04_01;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ntext\nYES\n\nNA\n\n\n\ncounty_name\ntext\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\ndepartment_name\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\nsearch_conducted\ndouble\nYES\n\nNA\n\n\n\nraw_dept_lvl\ntext\nYES\n\nNA\n\n\n\nraw_dept\ntext\nYES\n\nNA\n\n\n\nraw_Race\ntext\nYES\n\nNA\nDESCRIBE ma_statewide_2020_04_01;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ntext\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\ncounty_name\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\narrest_made\ndouble\nYES\n\nNA\n\n\n\ncitation_issued\ndouble\nYES\n\nNA\nDESCRIBE va_statewide_2020_04_01;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ntext\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\ncounty_name\ntext\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nofficer_id_hash\ntext\nYES\n\nNA\n\n\n\nofficer_race\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\nsearch_conducted\ndouble\nYES\n\nNA\n\n\n\nraw_officer_race\ntext\nYES\n\nNA"
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "Racial Differences in Traffic-Stop Search Rates across Nebraska, Masaschusetts, and Virginia",
    "section": "Introduction",
    "text": "Introduction\nFor this week’s project, I wanted to examine the racial differences in traffic-stop search rates across Nebraska, Massachusetts, and Virginia using our given database, Stanford’s Open Policing Project. The reason why I chose these states were because I’ve lived in them, and they also happen to be vastly politically different from each other, so I thought it could be interesting. The first state I immigrated to when I came to the U.S. was Nebraska, then I lived in Boston, MA for a bit, before settling down in VA for high school and most of my middle school years. Nebraska is a state that is very red, Massachusetts is a state that is very blue, and Virginia is a swing state, so I thought that maybe there would be a correlation between the proportion of searches conducted to the race of the driver. After using DESCRIBE on all 3 of the statewide tables, I found out that there were enough consistent variable sets (subject race, search conducted, and date) which would allow me to combine the data with union. Below, I’ll have created a unified dataset that summarizes search behavior by race and year for each state– with my primary goal being to determine whether or not search rates differ by racial group, how the differences vary across states, and whether or not they change over time (for example, Virginia goes pretty back and forth every year between R and D, and Nebraska’s 2nd district swings R and D a lot as well)."
  },
  {
    "objectID": "project5.html#code",
    "href": "project5.html#code",
    "title": "Racial Differences in Traffic-Stop Search Rates across Nebraska, Masaschusetts, and Virginia",
    "section": "Code",
    "text": "Code\n\n\nSELECT\n  'Nebraska' AS state,\n  subject_race,\n  type,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) AS searches,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) / COUNT(*) AS search_rate\nFROM ne_statewide_2020_04_01\nWHERE `date` LIKE '2015%'\n  AND subject_race IS NOT NULL\nGROUP BY subject_race, type\nHAVING COUNT(*) &gt;= 100\nORDER BY subject_race, type;\n\n\n\nSELECT\n  'Massachusetts' AS state,\n  subject_race,\n  type,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) AS searches,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) / COUNT(*) AS search_rate\nFROM ma_statewide_2020_04_01\nWHERE `date` LIKE '2015%'\n  AND subject_race IS NOT NULL\nGROUP BY subject_race, type\nHAVING COUNT(*) &gt;= 100\nORDER BY subject_race, type;\n\n\n\n\nSELECT\n  'Virginia' AS state,\n  subject_race,\n  type,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) AS searches,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) / COUNT(*) AS search_rate\nFROM va_statewide_2020_04_01\nWHERE `date` LIKE '2015%'\n  AND subject_race IS NOT NULL\nGROUP BY subject_race, type\nHAVING COUNT(*) &gt;= 100\nORDER BY subject_race, type;\n\n\n#| echo: true\n\nstate_race_2015 &lt;- bind_rows(\n  ne_race_2015,\n  ma_race_2015,\n  va_race_2015\n)\n\nstate_race_2015\n\n           state           subject_race      type total_stops searches\n1       Nebraska asian/pacific islander vehicular        7483      142\n2       Nebraska                  black vehicular       35257     2074\n3       Nebraska               hispanic vehicular       43274     2022\n4       Nebraska                  other vehicular        9132      411\n5       Nebraska                  white vehicular      408493     9430\n6  Massachusetts asian/pacific islander vehicular       18489       91\n7  Massachusetts                  black vehicular       37428      463\n8  Massachusetts               hispanic vehicular       35799      743\n9  Massachusetts                  other vehicular        1006        5\n10 Massachusetts                unknown vehicular         731        0\n11 Massachusetts                  white vehicular      249988     1708\n12      Virginia asian/pacific islander vehicular       10950       62\n13      Virginia                  black vehicular      111636     1405\n14      Virginia               hispanic vehicular       28526      489\n15      Virginia                  other vehicular       12396       73\n16      Virginia                unknown vehicular        7083       19\n17      Virginia                  white vehicular      282002     5009\n   search_rate\n1       0.0190\n2       0.0588\n3       0.0467\n4       0.0450\n5       0.0231\n6       0.0049\n7       0.0124\n8       0.0208\n9       0.0050\n10      0.0000\n11      0.0068\n12      0.0057\n13      0.0126\n14      0.0171\n15      0.0059\n16      0.0027\n17      0.0178\n\nrace_summary_2015 &lt;- state_race_2015 |&gt;\n  group_by(state, subject_race) |&gt;\n  summarize(\n    total_stops    = sum(total_stops),\n    total_searches = sum(searches),\n    overall_search_rate = total_searches / total_stops,\n    .groups = \"drop\"\n  )\n\nrace_summary_2015\n\n# A tibble: 17 × 5\n   state         subject_race     total_stops total_searches overall_search_rate\n   &lt;chr&gt;         &lt;chr&gt;                &lt;int64&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n 1 Massachusetts asian/pacific i…       18489             91             0.00492\n 2 Massachusetts black                  37428            463             0.0124 \n 3 Massachusetts hispanic               35799            743             0.0208 \n 4 Massachusetts other                   1006              5             0.00497\n 5 Massachusetts unknown                  731              0             0      \n 6 Massachusetts white                 249988           1708             0.00683\n 7 Nebraska      asian/pacific i…        7483            142             0.0190 \n 8 Nebraska      black                  35257           2074             0.0588 \n 9 Nebraska      hispanic               43274           2022             0.0467 \n10 Nebraska      other                   9132            411             0.0450 \n11 Nebraska      white                 408493           9430             0.0231 \n12 Virginia      asian/pacific i…       10950             62             0.00566\n13 Virginia      black                 111636           1405             0.0126 \n14 Virginia      hispanic               28526            489             0.0171 \n15 Virginia      other                  12396             73             0.00589\n16 Virginia      unknown                 7083             19             0.00268\n17 Virginia      white                 282002           5009             0.0178 \n\n\n\n#| echo: true\n\nggplot(race_summary_2015,\n       aes(x = subject_race, y = overall_search_rate, fill = state)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = function(x) paste0(round(100 * x, 1), \"%\")) +\n  labs(\n    title = \"Search Rates by Driver Race in 2015\",\n    x = \"Driver's Race\",\n    y = \"Search Rate (% of stops with a search)\",\n    fill = \"State\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis bar graph displays the search rate, or the % of stops that resulted in a search, for racial groups across Nebraska, Massachusetts, and Virginia in 2015. From the graph, I can see some patterns emerging– dirvers of color would experiecne the highest search rates, with black drivers in NE, and hispanic drivers in MA and VA. Originally, when I looked at the graphs, and saw that NE searched the highest % of black and hispanic drivers, I went, “of course the republican state,” but, looking more closely, they just search way more in general compared to the other two states, so I’ll definitely have to look more into it to draw conclusions. However, overall, the differences indicate that, once stopped, drivers of different racial groups do not face the same likelihood of being searched. The consistency of this pattern across three states suggests that disparities in post-stop treatment are not isolated to a single jurisdiction but it could reflect broader systemic practices.\n\n#| echo: true\n\nggplot(race_summary_2015,\n       aes(x = subject_race, y = total_stops, fill = state)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = function(x) format(x, big.mark = \",\")) +\n  labs(\n    title = \"Total Traffic Stops by Driver Race in 2015\",\n    x = \"Driver's Race\",\n    y = \"Number of Stops\",\n    fill = \"State\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThis bar chart helps us show the total number of traffic stops by race in NE, MA, and VA during 2015. The plot highlights substantial differences in stop volume across racial groups. In each state, white drivers make up the majority of the stops, which makes sense because the U.S. is a country with a white majority population. By contrast, Black, Hispanic, Asian/Pacific Islander, and “Other” and “Unknown” categories represent much smaller portions of overall stop counts. These differences in stop volume help contextualize the search rates observed in the first figure. Even though white drivers have relatively low search rates, their large number of total stops means they account for a high absolute number of searches. Racial groups with higher search rates may have far fewer total stops but still experience disproportionately high search likelihoods once stopped."
  },
  {
    "objectID": "project5.html#concluding-thoughts",
    "href": "project5.html#concluding-thoughts",
    "title": "Racial Differences in Traffic-Stop Search Rates across Nebraska, Masaschusetts, and Virginia",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nSo I deviated a bit from my original plan because of some logistical issues. When I ran the code I wanted to run to union all of the 3 tables across the years from 2011 all the way through 2015, it actually move at a dinosaur pace. Like I wasted 45 minutes of my life running that chunk, and I wasn’t going to sit there and wait for that to run each time I was debugging my r-code for the graphs later on. So, I decided to limit my work to the most recent year (2015) and also got rid of all the whitespace in my work for the SQL section (sorry for the bad syntax) so I could maximize the optimization possible for the code.I then made the sql tables into an output variable, and joined them together with rbind() rather than union. For the graphs, I couldn’t make them proportionally adjusted as well because I would need the number of people of each race in the state, the number of licensed drivers by race, and none of those variables were all shared equally among the SQL tables, if they even were available. So, my two plots of stop volume and search rate conditional on being stopped were the graphical analyses I went with – if I were to look into this more in the future, I’d look into interpreting search rates only within the population of stopped drivers, because mine currently only answers “given that a driver was stopped, what is the probability they were searched.”\nAll in all, the results show clear racial differences in both the likelihood of being searched during a traffic stop and the volume of stops across three states. While white drivers make up the majority of stops in Nebraska, Massachusetts, and Virginia, Black and Hispanic drivers consistently face higher search rates once stopped, indicating disparities in post-stop treatment rather than in stop frequency alone. Because the Stanford Open Policing Project does not provide population denominators or licensed-driver demographics, the findings reflect conditional outcomes (what happens ONCE someone is stopped?) rather than per-capita policing rates. Nonetheless, I believe that the consistency of the pattern across multiple states suggests that racial disparities in search practices are systemic rather than isolated. It’s important continue transparency and standarized data collection as we continue our public oversight in policing.\n\n#| echo: false\n\nDBI::dbDisconnect(con_traffic)"
  },
  {
    "objectID": "project5.html#citations",
    "href": "project5.html#citations",
    "title": "Racial Differences in Traffic-Stop Search Rates across Nebraska, Masaschusetts, and Virginia",
    "section": "Citations",
    "text": "Citations\nThe data used in my project comes from the Stanford Open Policing Project, which was original assembled and released as a part of Pierson et Al.’s work in “A large-scale analysis of racial disparities in police stops across the United States.” I’ll leave the link to that here.We can access the data through Stanford’s public data portal at the following link: https://openpolicing.stanford.edu/."
  },
  {
    "objectID": "proj6.html#website-overview",
    "href": "proj6.html#website-overview",
    "title": "Website Presentation",
    "section": "Website Overview",
    "text": "Website Overview\nThis is a sample sentence."
  },
  {
    "objectID": "proj6.html",
    "href": "proj6.html",
    "title": "Website Presentation",
    "section": "",
    "text": "Quantifying Emily Dickinson’s Poetry\n\nFull-text analysis of all poems from the DickinsonPoems R package\n\nTry and identify characteristics of Dickinson’s writing through regex & text processing.\nFocuses:\n\nVocabulary Patterns\nPunctuation Behavior\nMotifs (ex: death, nature, time)\nPersonal language (“my ___, I”)\n\nAim to study how Dickinson writes across her entire body of work, rather than attempt to interpret individual poems."
  },
  {
    "objectID": "proj6.html#introduction",
    "href": "proj6.html#introduction",
    "title": "Website Presentation",
    "section": "Introduction",
    "text": "Introduction\nQuantifying Emily Dickinson’s Poetry\n\nFull-text analysis of all poems from the DickinsonPoems R package\n\nTry and identify characteristics of Dickinson’s writing through regex & text processing.\nFocuses:\n\nVocabulary Patterns\nPunctuation Behavior\nMotifs (ex: death, nature, time)\nPersonal language (“my ___, I”)\n\nAim to study how Dickinson writes across her entire body of work, rather than attempt to interpret individual poems."
  },
  {
    "objectID": "proj6.html#original-analysis",
    "href": "proj6.html#original-analysis",
    "title": "Website Presentation",
    "section": "Original Analysis",
    "text": "Original Analysis\nVersion 1 Overview\n\nUsed regular expressions to extract:\n\nDeath-related vocabulary (appears in text 4, 8, 9)\nFour-digit, year like numbers\nWords following “my” through lookbehind (soul, reach)\n\nPlotted:\n\nMost frequent word counts\nRelationship between poem length and punctuation count"
  },
  {
    "objectID": "proj6.html#new-analysis",
    "href": "proj6.html#new-analysis",
    "title": "Website Presentation",
    "section": "New Analysis",
    "text": "New Analysis\nRefinements in Version 2\n\nCleaned text by removing:\n\nContractions\nApostrophes and possessive endings\nOne-letter fragments (ex: s after it’s, or t after don’t)\n\nRemoved common pronouns, articles, and auxiliary verbs (stop words)\nTry and limit the words down to limited, meaningful text"
  },
  {
    "objectID": "proj6.html#semantic-interpretation",
    "href": "proj6.html#semantic-interpretation",
    "title": "Website Presentation",
    "section": "Semantic Interpretation",
    "text": "Semantic Interpretation\nPoetic Functions of Common Words\n\n\n\n\n\n\n\nWord\nFunction\n\n\n\n\nlittle\nAttention to size and length, precision, measured perception\n\n\nday / life\nBoundaries, duration, temporal framing\n\n\ntill / will\nAnticipation, incompleteness, inevitability, deferred action\n\n\naway\nDistance, separation, loss, withdrawal\n\n\n\nDickinson’s writing is empowered not through elaborate vocabulary, but through framing and restricted language. She builds meaning by holding back, delaying, or measuring experience."
  },
  {
    "objectID": "proj6.html#punctuation-analysis",
    "href": "proj6.html#punctuation-analysis",
    "title": "Website Presentation",
    "section": "Punctuation Analysis",
    "text": "Punctuation Analysis\nPoem Length and Punctuation Frequency\nObservations: - Longer poems generally contain more punctuation - However, there is significant variation beyond the predicted trend\nPunctuation functions stylistically rather than mechanically. In many poems, punctuation marks (particularly dashes) serve as rhythmic and expressive devices that shape hesitation, emphasis, and tone."
  },
  {
    "objectID": "proj6.html#future-work",
    "href": "proj6.html#future-work",
    "title": "Website Presentation",
    "section": "Future Work",
    "text": "Future Work\nPossible extensions of this analysis include:\n\nSeparate classification of em dashes distinct from other punctuation\nTopic modeling to group poems by thematic patterns such as death, nature, faith, or memory\nSentiment analysis to measure emotional levels in the poems.\nTemporal comparison of early and late poems to detect stylistic changes/evolution."
  },
  {
    "objectID": "proj6.html#closing",
    "href": "proj6.html#closing",
    "title": "Website Presentation",
    "section": "Closing",
    "text": "Closing\nEmily Dickinson’s poetry shows that expressive force can emerge from small words, restraint, and punctuation. Her style depends not on complex language, but on how fragments, pauses, and concise terms carry emotional and philosophical weight.\nThank you."
  },
  {
    "objectID": "proj6.html#thank-you",
    "href": "proj6.html#thank-you",
    "title": "Website Presentation",
    "section": "Thank You!",
    "text": "Thank You!"
  }
]