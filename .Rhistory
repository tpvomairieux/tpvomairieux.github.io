poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize_filtered |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words %in% stop_words) |>
filter(str_length(words) > 1)
#| echo: true
# Regular Expressions
death_regexp <- "\\b(death|die|grave|funeral|corpse|end|demise|passing)\\b"
year_regexp <- "\\b(10|11|12|13|14|15|16|17|18|19|20)\\d{2}\\b"
# LookAround (Look Behind)
after_word_my_regexp <- "(?<=\\bmy\\s)[A-Za-z']+"
poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize_filtered |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(words = str_split(str_to_lower(text), "\\W+")) |>
unnest(words) |>
filter(words != "", !str_detect(words, "^\\d+$"))
#| echo: true
# Regular Expressions
death_regexp <- "\\b(death|die|grave|funeral|corpse|end|demise|passing)\\b"
year_regexp <- "\\b(10|11|12|13|14|15|16|17|18|19|20)\\d{2}\\b"
# LookAround (Look Behind)
after_word_my_regexp <- "(?<=\\bmy\\s)[A-Za-z']+"
poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words %in% stop_words) |>
filter(str_length(words) > 1)
#| echo: true
# Regular Expressions
death_regexp <- "\\b(death|die|grave|funeral|corpse|end|demise|passing)\\b"
year_regexp <- "\\b(10|11|12|13|14|15|16|17|18|19|20)\\d{2}\\b"
# LookAround (Look Behind)
after_word_my_regexp <- "(?<=\\bmy\\s)[A-Za-z']+"
poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize_filtered |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words == stop_words) |>
filter(str_length(words) > 1)
#| echo: true
# Regular Expressions
death_regexp <- "\\b(death|die|grave|funeral|corpse|end|demise|passing)\\b"
year_regexp <- "\\b(10|11|12|13|14|15|16|17|18|19|20)\\d{2}\\b"
# LookAround (Look Behind)
after_word_my_regexp <- "(?<=\\bmy\\s)[A-Za-z']+"
poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize_filtered |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words in stop_words) |>
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words |>in|> stop_words) |>
#| include: false
#| message: false
#| warning: false
library(tidyverse)
library(DickinsonPoems)
#| include: false
poem_ids <- list_poems()
poems_lines <- data.frame(poem_id = poem_ids) |>
mutate(lines = map(poem_id, get_poem)) |>
tidyr::unnest_longer(lines) |>
rename(line_text = lines)
poems <- poems_lines |>
group_by(poem_id) |>
summarize(
text = str_c(line_text, collapse = " "),
n_lines = n(),
.groups = "drop"
)
tokenize <- poems |>
mutate(
text_clean = str_replace_all(text, "\\b[A-Za-z]+'t\\b", ""),
text_clean = str_replace_all(text_clean, "n't\\b", ""),
text_clean = str_replace_all(text_clean, "'s\\b", ""),
text_clean = str_replace_all(text_clean, "'", "")
) |>
mutate(words = str_split(str_to_lower(text_clean), "[^a-z]+")) |>
unnest(words) |>
filter(words != "")
stop_words <- c(
"the", "a", "an", "and", "to", "of", "in", "is", "it", "that", "for",
"on", "as", "with", "at", "by", "from", "this", "be", "or", "not",
"but", "are", "was", "were", "so", "if", "up", "out", "then",
"than", "there", "here", "such","i", "me", "my", "you", "your",
"she", "he", "we", "they", "them", "his", "her", "hers", "its",
"our", "ours", "their", "theirs", "when", "where", "why", "what",
"which", "who", "whom", "how", "have", "has", "had", "do", "does",
"did", "can", "could", "would", "should", "may", "might", "must",
"like", "no", "all", "one", "upon", "into", "over", "under",
"ever", "never", "again", "still", "just", "only", "very"
)
tokenize_filtered <- tokenize |>
filter(!words %in% stop_words) |>
filter(str_length(words) > 1)
#| echo: true
# Regular Expressions
death_regexp <- "\\b(death|die|grave|funeral|corpse|end|demise|passing)\\b"
year_regexp <- "\\b(10|11|12|13|14|15|16|17|18|19|20)\\d{2}\\b"
# LookAround (Look Behind)
after_word_my_regexp <- "(?<=\\bmy\\s)[A-Za-z']+"
poems <- poems |>
mutate(
punctuation_count = str_count(text, "[[:punct:]]"),
mentions_death = str_detect(str_to_lower(text), death_regexp),
years_found = str_extract_all(text, year_regexp),
after_word_my_regexp = str_extract(text, after_word_my_regexp)
)
poems |> select(poem_id, n_lines, punctuation_count, mentions_death, years_found, after_word_my_regexp) |> head(10)
#| echo: true
most_used_words <- tokenize_filtered |>
count(words, sort = TRUE) |>
slice_head(n = 10)
most_used_words |>
ggplot(aes(x = reorder(words, n), y = n)) +
geom_col(fill = "cornflowerblue") +
geom_text(aes(label = n),
size = 3) +
coord_flip() +
labs(
title = "Top 10 Most Frequent Words in Emily Dickinson’s Poems",
x = "Word",
y = "Number of Times Used"
) +
theme_light()
#| echo: true
ggplot(poems, aes(x = n_lines, y = punctuation_count)) +
geom_point(alpha = 0.5, color = "seagreen1") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
labs(
title = "Correlation Between Poem Length and Punctuation Count",
x = "Number of Lines in Poem",
y = "Frequency of Punctuation(s)"
) +
theme_light()
